{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\user\\Desktop\\Physical_AI_Book\\frontend\\sidebars.js","contentPath":"C:\\Users\\user\\Desktop\\Physical_AI_Book\\frontend\\docs","docs":[{"id":"about","title":"About Physical AI Book","description":"Mission","source":"@site/docs/about.md","sourceDirName":".","slug":"/about","permalink":"/docs/about","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/about.md","tags":[],"version":"current","sidebarPosition":99,"frontMatter":{"sidebar_position":99}},{"id":"conclusion","title":"Conclusion: Complete Physical AI Curriculum","description":"Congratulations! ðŸŽ‰","source":"@site/docs/conclusion.md","sourceDirName":".","slug":"/conclusion","permalink":"/docs/conclusion","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/conclusion.md","tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100}},{"id":"development-setup","title":"Development Setup","description":"This guide explains how to set up your development environment for contributing to the Physical AI Book project.","source":"@site/docs/development-setup.md","sourceDirName":".","slug":"/development-setup","permalink":"/docs/development-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/development-setup.md","tags":[],"version":"current","sidebarPosition":98,"frontMatter":{"sidebar_position":98}},{"id":"faq","title":"Frequently Asked Questions","description":"General","source":"@site/docs/faq.md","sourceDirName":".","slug":"/faq","permalink":"/docs/faq","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/faq.md","tags":[],"version":"current","sidebarPosition":100,"frontMatter":{"sidebar_position":100}},{"id":"intro","title":"Introduction to Physical AI","description":"Welcome to the Physical AI Book - an educational platform designed to teach you how to build AI systems that perceive, reason, and act in the physical world.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"Module 1: The Robotic Nervous System (ROS 2)","permalink":"/docs/module1/intro"}},{"id":"module1/basics","title":"ROS 2 Basics","description":"This section covers the fundamental concepts of Robot Operating System 2 (ROS 2).","source":"@site/docs/module1/basics.md","sourceDirName":"module1","slug":"/module1/basics","permalink":"/docs/module1/basics","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module1/basics.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 1: The Robotic Nervous System (ROS 2)","permalink":"/docs/module1/intro"},"next":{"title":"ROS 2 Nodes","permalink":"/docs/module1/nodes"}},{"id":"module1/intro","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Welcome to Module 1, where you'll learn the foundational concepts of Robot Operating System 2 (ROS 2) - the middleware that enables AI software to communicate with and control humanoid robots in simulated environments.","source":"@site/docs/module1/intro.md","sourceDirName":"module1","slug":"/module1/intro","permalink":"/docs/module1/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module1/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Physical AI","permalink":"/docs/intro"},"next":{"title":"ROS 2 Basics","permalink":"/docs/module1/basics"}},{"id":"module1/nodes","title":"ROS 2 Nodes","description":"In ROS 2, a node is a process that performs computation. Nodes are the fundamental building blocks of a ROS 2 system, and they can be distributed across multiple machines in a network.","source":"@site/docs/module1/nodes.md","sourceDirName":"module1","slug":"/module1/nodes","permalink":"/docs/module1/nodes","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module1/nodes.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Basics","permalink":"/docs/module1/basics"},"next":{"title":"ROS 2 Topics and Messages","permalink":"/docs/module1/topics"}},{"id":"module1/projects","title":"Module 1 Projects","description":"In this section, you'll work on hands-on projects that apply the concepts learned in this module.","source":"@site/docs/module1/projects.md","sourceDirName":"module1","slug":"/module1/projects","permalink":"/docs/module1/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module1/projects.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Topics and Messages","permalink":"/docs/module1/topics"},"next":{"title":"Module 2: Digital Twin (Gazebo & Unity)","permalink":"/docs/module2/intro"}},{"id":"module1/topics","title":"ROS 2 Topics and Messages","description":"Topics are named buses over which nodes exchange messages. Understanding topics is crucial for creating communication between different parts of your robotic system.","source":"@site/docs/module1/topics.md","sourceDirName":"module1","slug":"/module1/topics","permalink":"/docs/module1/topics","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module1/topics.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Nodes","permalink":"/docs/module1/nodes"},"next":{"title":"Module 1 Projects","permalink":"/docs/module1/projects"}},{"id":"module2/gazebo","title":"Gazebo Simulation Environment","description":"Gazebo is a 3D dynamic simulator that supports accurate simulation of robots in complex indoor and outdoor environments. It's widely used in robotics research and development.","source":"@site/docs/module2/gazebo.md","sourceDirName":"module2","slug":"/module2/gazebo","permalink":"/docs/module2/gazebo","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module2/gazebo.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 2: Digital Twin (Gazebo & Unity)","permalink":"/docs/module2/intro"},"next":{"title":"Unity Simulation Environment","permalink":"/docs/module2/unity"}},{"id":"module2/integration","title":"Integrating Simulation with ROS 2","description":"In this section, we'll explore how to tightly integrate Gazebo simulation with your ROS 2 nodes from Module 1, creating a seamless development workflow.","source":"@site/docs/module2/integration.md","sourceDirName":"module2","slug":"/module2/integration","permalink":"/docs/module2/integration","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module2/integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Unity Simulation Environment","permalink":"/docs/module2/unity"},"next":{"title":"Module 2 Projects","permalink":"/docs/module2/projects"}},{"id":"module2/intro","title":"Module 2: Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2, where you'll learn to create digital twins of humanoid robots and their environments to validate control logic, physics, and interactions.","source":"@site/docs/module2/intro.md","sourceDirName":"module2","slug":"/module2/intro","permalink":"/docs/module2/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module2/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 1 Projects","permalink":"/docs/module1/projects"},"next":{"title":"Gazebo Simulation Environment","permalink":"/docs/module2/gazebo"}},{"id":"module2/projects","title":"Module 2 Projects","description":"In this section, you'll work on hands-on projects that apply the concepts learned about digital twins and simulation environments.","source":"@site/docs/module2/projects.md","sourceDirName":"module2","slug":"/module2/projects","permalink":"/docs/module2/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module2/projects.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Integrating Simulation with ROS 2","permalink":"/docs/module2/integration"},"next":{"title":"Module 3: AI-Robot Brain (NVIDIA Isaac)","permalink":"/docs/module3/intro"}},{"id":"module2/unity","title":"Unity Simulation Environment","description":"Unity is a powerful 3D development platform that can be used for creating high-fidelity robotic simulations. While traditionally used for gaming, Unity's physics engine and rendering capabilities make it excellent for robotics simulation.","source":"@site/docs/module2/unity.md","sourceDirName":"module2","slug":"/module2/unity","permalink":"/docs/module2/unity","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module2/unity.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Simulation Environment","permalink":"/docs/module2/gazebo"},"next":{"title":"Integrating Simulation with ROS 2","permalink":"/docs/module2/integration"}},{"id":"module3/intro","title":"Module 3: AI-Robot Brain (NVIDIA Isaac)","description":"Welcome to Module 3, where you'll build the AI brain for humanoid robots using NVIDIA Isaac Sim and Isaac ROS with photorealistic simulation and synthetic data.","source":"@site/docs/module3/intro.md","sourceDirName":"module3","slug":"/module3/intro","permalink":"/docs/module3/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module3/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 2 Projects","permalink":"/docs/module2/projects"},"next":{"title":"NVIDIA Isaac Sim","permalink":"/docs/module3/isaac"}},{"id":"module3/isaac","title":"NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a robotics simulation application and ecosystem that accelerates AI training and robotics development. Built on NVIDIA Omniverse, it provides photorealistic simulation capabilities essential for Physical AI development.","source":"@site/docs/module3/isaac.md","sourceDirName":"module3","slug":"/module3/isaac","permalink":"/docs/module3/isaac","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module3/isaac.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: AI-Robot Brain (NVIDIA Isaac)","permalink":"/docs/module3/intro"},"next":{"title":"Perception Pipelines in Isaac Sim","permalink":"/docs/module3/perception"}},{"id":"module3/navigation","title":"Navigation and Path Planning","description":"Navigation and path planning form the foundation of mobile robot autonomy. In this section, we'll explore how to implement these capabilities using Isaac Sim and Isaac ROS components.","source":"@site/docs/module3/navigation.md","sourceDirName":"module3","slug":"/module3/navigation","permalink":"/docs/module3/navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module3/navigation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Perception Pipelines in Isaac Sim","permalink":"/docs/module3/perception"},"next":{"title":"Module 3 Projects","permalink":"/docs/module3/projects"}},{"id":"module3/perception","title":"Perception Pipelines in Isaac Sim","description":"Perception is a critical component of the AI-Robot brain, enabling robots to understand their environment. Isaac Sim provides advanced tools for developing and testing perception pipelines.","source":"@site/docs/module3/perception.md","sourceDirName":"module3","slug":"/module3/perception","permalink":"/docs/module3/perception","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module3/perception.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Sim","permalink":"/docs/module3/isaac"},"next":{"title":"Navigation and Path Planning","permalink":"/docs/module3/navigation"}},{"id":"module3/projects","title":"Module 3 Projects","description":"In this section, you'll work on hands-on projects that apply the concepts learned about AI-Robot brains, Isaac Sim, and perception/navigation systems.","source":"@site/docs/module3/projects.md","sourceDirName":"module3","slug":"/module3/projects","permalink":"/docs/module3/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module3/projects.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Navigation and Path Planning","permalink":"/docs/module3/navigation"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/module4/intro"}},{"id":"module4/conclusion","title":"Module 4 Conclusion","description":"Summary of Vision-Language-Action Systems","source":"@site/docs/module4/conclusion.md","sourceDirName":"module4","slug":"/module4/conclusion","permalink":"/docs/module4/conclusion","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/conclusion.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 Projects: Vision-Language-Action Integration","permalink":"/docs/module4/projects"}},{"id":"module4/integration","title":"VLA System Integration","description":"In this section, we'll integrate all components from previous modules to create a complete Vision-Language-Action system that can understand natural language commands and execute them in simulation.","source":"@site/docs/module4/integration.md","sourceDirName":"module4","slug":"/module4/integration","permalink":"/docs/module4/integration","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/integration.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action (VLA) Systems","permalink":"/docs/module4/vla"},"next":{"title":"Module 4 Projects: Vision-Language-Action Integration","permalink":"/docs/module4/projects"}},{"id":"module4/intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Welcome to Module 4, where you'll integrate LLMs, speech recognition, and cognitive planning to enable humanoid robots to perform tasks based on human instructions.","source":"@site/docs/module4/intro.md","sourceDirName":"module4","slug":"/module4/intro","permalink":"/docs/module4/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Projects","permalink":"/docs/module3/projects"},"next":{"title":"Vision-Language-Action (VLA) Systems","permalink":"/docs/module4/vla"}},{"id":"module4/projects","title":"Module 4 Projects: Vision-Language-Action Integration","description":"In this section, you'll work on comprehensive projects that integrate all the concepts learned in the previous modules to create complete Vision-Language-Action systems.","source":"@site/docs/module4/projects.md","sourceDirName":"module4","slug":"/module4/projects","permalink":"/docs/module4/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/projects.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"VLA System Integration","permalink":"/docs/module4/integration"},"next":{"title":"Module 4 Conclusion","permalink":"/docs/module4/conclusion"}},{"id":"module4/vla","title":"Vision-Language-Action (VLA) Systems","description":"Vision-Language-Action (VLA) systems represent the integration of computer vision, natural language processing, and robotic action execution. These systems enable robots to understand human instructions, perceive their environment, and execute complex tasks.","source":"@site/docs/module4/vla.md","sourceDirName":"module4","slug":"/module4/vla","permalink":"/docs/module4/vla","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/vla.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/module4/intro"},"next":{"title":"VLA System Integration","permalink":"/docs/module4/integration"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"category","label":"Module 1: ROS 2 Fundamentals","items":[{"type":"doc","id":"module1/intro"},{"type":"doc","id":"module1/basics"},{"type":"doc","id":"module1/nodes"},{"type":"doc","id":"module1/topics"},{"type":"doc","id":"module1/projects"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Digital Twin","items":[{"type":"doc","id":"module2/intro"},{"type":"doc","id":"module2/gazebo"},{"type":"doc","id":"module2/unity"},{"type":"doc","id":"module2/integration"},{"type":"doc","id":"module2/projects"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: AI-Robot Brain","items":[{"type":"doc","id":"module3/intro"},{"type":"doc","id":"module3/isaac"},{"type":"doc","id":"module3/perception"},{"type":"doc","id":"module3/navigation"},{"type":"doc","id":"module3/projects"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action","items":[{"type":"doc","id":"module4/intro"},{"type":"doc","id":"module4/vla"},{"type":"doc","id":"module4/integration"},{"type":"doc","id":"module4/projects"},{"type":"doc","id":"module4/conclusion"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.js"},{"type":"jsx","permalink":"/layout","source":"@site/src/pages/layout.js"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"global-chat-plugin":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}