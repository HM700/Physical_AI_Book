{
  "id": "module4/vla",
  "title": "Vision-Language-Action (VLA) Systems",
  "description": "Vision-Language-Action (VLA) systems represent the integration of computer vision, natural language processing, and robotic action execution. These systems enable robots to understand human instructions, perceive their environment, and execute complex tasks.",
  "source": "@site/docs/module4/vla.md",
  "sourceDirName": "module4",
  "slug": "/module4/vla",
  "permalink": "/docs/module4/vla",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/vla.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 2,
  "frontMatter": {
    "sidebar_position": 2
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Module 4: Vision-Language-Action (VLA)",
    "permalink": "/docs/module4/intro"
  },
  "next": {
    "title": "VLA System Integration",
    "permalink": "/docs/module4/integration"
  }
}