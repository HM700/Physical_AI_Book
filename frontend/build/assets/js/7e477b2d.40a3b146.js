"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[313],{6243(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module3/projects","title":"Module 3 Projects","description":"In this section, you\'ll work on hands-on projects that apply the concepts learned about AI-Robot brains, Isaac Sim, and perception/navigation systems.","source":"@site/docs/module3/projects.md","sourceDirName":"module3","slug":"/module3/projects","permalink":"/Physical-AI-Book/docs/module3/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module3/projects.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Navigation and Path Planning","permalink":"/Physical-AI-Book/docs/module3/navigation"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Book/docs/module4/intro"}}');var a=t(4848),o=t(8453);const s={sidebar_position:5},r="Module 3 Projects",l={},c=[{value:"Project 1: Complete AI-Robot Brain System",id:"project-1-complete-ai-robot-brain-system",level:2},{value:"Objective",id:"objective",level:3},{value:"Requirements",id:"requirements",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Testing the Project",id:"testing-the-project",level:3},{value:"Project 2: Isaac Sim Perception Training Pipeline",id:"project-2-isaac-sim-perception-training-pipeline",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Requirements",id:"requirements-1",level:3},{value:"Project 3: Multi-Agent AI-Robot Coordination",id:"project-3-multi-agent-ai-robot-coordination",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Requirements",id:"requirements-2",level:3},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"module-3-projects",children:"Module 3 Projects"})}),"\n",(0,a.jsx)(n.p,{children:"In this section, you'll work on hands-on projects that apply the concepts learned about AI-Robot brains, Isaac Sim, and perception/navigation systems."}),"\n",(0,a.jsx)(n.h2,{id:"project-1-complete-ai-robot-brain-system",children:"Project 1: Complete AI-Robot Brain System"}),"\n",(0,a.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,a.jsx)(n.p,{children:"Create a complete AI-Robot brain system that integrates perception, navigation, and decision-making in Isaac Sim."}),"\n",(0,a.jsx)(n.h3,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create a robot with multiple sensors (RGB-D camera, IMU, wheel encoders)"}),"\n",(0,a.jsx)(n.li,{children:"Implement perception pipeline with object detection and semantic segmentation"}),"\n",(0,a.jsx)(n.li,{children:"Implement navigation system with path planning and obstacle avoidance"}),"\n",(0,a.jsx)(n.li,{children:"Create decision-making system for task execution"}),"\n",(0,a.jsx)(n.li,{children:"Integrate with Isaac Sim for photorealistic simulation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create the robot configuration"})," (",(0,a.jsx)(n.code,{children:"config/ai_robot.urdf"}),"):"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="ai_robot">\n  \x3c!-- Base link --\x3e\n  <link name="base_footprint">\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <joint name="base_joint" type="fixed">\n    <parent link="base_footprint"/>\n    <child link="base_link"/>\n    <origin xyz="0 0 0.2" rpy="0 0 0"/>\n  </joint>\n\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.3" length="0.2"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.3" length="0.2"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="20.0"/>\n      <inertia ixx="0.416666" ixy="0" ixz="0" iyy="0.416666" iyz="0" izz="0.9"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Wheels --\x3e\n  <link name="wheel_fl">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <link name="wheel_fr">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <link name="wheel_bl">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <link name="wheel_br">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <origin rpy="1.5708 0 0"/>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.005" ixy="0" ixz="0" iyy="0.005" iyz="0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Joints --\x3e\n  <joint name="wheel_fl_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_fl"/>\n    <origin xyz="0.2 0.2 -0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  <joint name="wheel_fr_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_fr"/>\n    <origin xyz="0.2 -0.2 -0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  <joint name="wheel_bl_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_bl"/>\n    <origin xyz="-0.2 0.2 -0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  <joint name="wheel_br_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_br"/>\n    <origin xyz="-0.2 -0.2 -0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n  </joint>\n\n  \x3c!-- Sensors --\x3e\n  <link name="imu_link">\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="1e-6" ixy="0" ixz="0" iyy="1e-6" iyz="0" izz="1e-6"/>\n    </inertial>\n  </link>\n\n  <joint name="imu_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n    <origin xyz="0 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 1"/>\n      </material>\n    </visual>\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="1e-6" ixy="0" ixz="0" iyy="1e-6" iyz="0" izz="1e-6"/>\n    </inertial>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.25 0 0.05" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugins --\x3e\n  <gazebo reference="base_link">\n    <material>Gazebo/Blue</material>\n  </gazebo>\n\n  <gazebo reference="wheel_fl">\n    <mu1>10.0</mu1>\n    <mu2>10.0</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n    <material>Gazebo/Black</material>\n  </gazebo>\n\n  <gazebo reference="wheel_fr">\n    <mu1>10.0</mu1>\n    <mu2>10.0</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n    <material>Gazebo/Black</material>\n  </gazebo>\n\n  <gazebo reference="wheel_bl">\n    <mu1>10.0</mu1>\n    <mu2>10.0</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n    <material>Gazebo/Black</material>\n  </gazebo>\n\n  <gazebo reference="wheel_br">\n    <mu1>10.0</mu1>\n    <mu2>10.0</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n    <material>Gazebo/Black</material>\n  </gazebo>\n\n  \x3c!-- Differential drive plugin --\x3e\n  <gazebo>\n    <plugin name="drive_controller" filename="libgazebo_ros_diff_drive.so">\n      <left_joint>wheel_fl_joint</left_joint>\n      <right_joint>wheel_fr_joint</right_joint>\n      <wheel_separation>0.4</wheel_separation>\n      <wheel_diameter>0.2</wheel_diameter>\n      <command_topic>cmd_vel</command_topic>\n      <odometry_topic>odom</odometry_topic>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_footprint</robot_base_frame>\n      <publish_odom>true</publish_odom>\n      <publish_wheel_tf>true</publish_wheel_tf>\n      <publish_odom_tf>true</publish_odom_tf>\n      <legacy_mode>false</legacy_mode>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- IMU Sensor --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <visualize>true</visualize>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Camera Sensor --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera_sensor" type="camera">\n      <update_rate>30</update_rate>\n      <camera name="head_camera">\n        <horizontal_fov>1.3962634</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>100</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>camera_link</frame_name>\n        <topic_name>camera/image_raw</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n</robot>\n'})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create the AI-Robot Brain Node"}),":"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport tensorflow as tf\nfrom collections import deque\nimport threading\n\nclass AIRobotBrain(Node):\n    def __init__(self):\n        super().__init__('ai_robot_brain')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, '/detections', self.detection_callback, 10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.behavior_pub = self.create_publisher(String, '/behavior_status', 10)\n\n        # Robot state\n        self.current_pose = None\n        self.current_twist = None\n        self.imu_data = None\n        self.scan_data = None\n        self.last_image = None\n        self.detections = []\n\n        # AI brain components\n        self.perception_system = PerceptionSystem()\n        self.navigation_system = NavigationSystem()\n        self.decision_maker = DecisionMaker()\n\n        # Task queue\n        self.task_queue = deque()\n        self.current_task = None\n\n        # State machine\n        self.brain_state = 'IDLE'  # IDLE, PERCEIVING, NAVIGATING, EXECUTING, WAITING\n\n        # Timers\n        self.brain_timer = self.create_timer(0.1, self.brain_loop)\n\n        # Threading for heavy computations\n        self.lock = threading.Lock()\n\n    def image_callback(self, msg):\n        \"\"\"Process camera images for perception.\"\"\"\n        with self.lock:\n            self.last_image = msg\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data.\"\"\"\n        with self.lock:\n            self.imu_data = msg\n\n    def odom_callback(self, msg):\n        \"\"\"Process odometry data.\"\"\"\n        with self.lock:\n            self.current_pose = msg.pose.pose\n            self.current_twist = msg.twist.twist\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan data.\"\"\"\n        with self.lock:\n            self.scan_data = msg\n\n    def detection_callback(self, msg):\n        \"\"\"Process object detections.\"\"\"\n        with self.lock:\n            self.detections = msg.detections\n\n    def brain_loop(self):\n        \"\"\"Main AI brain state machine.\"\"\"\n        with self.lock:\n            if self.brain_state == 'IDLE':\n                # Check for tasks in queue\n                if self.task_queue:\n                    self.current_task = self.task_queue.popleft()\n                    self.brain_state = 'PERCEIVING'\n                    self.get_logger().info(f'Starting task: {self.current_task}')\n\n            elif self.brain_state == 'PERCEIVING':\n                # Process sensor data\n                if self.last_image:\n                    # Run perception pipeline\n                    perception_result = self.perception_system.process(\n                        self.last_image,\n                        self.scan_data,\n                        self.imu_data\n                    )\n\n                    # Update navigation system with perception\n                    self.navigation_system.update_environment(perception_result)\n\n                    # Decide next action\n                    next_action = self.decision_maker.decide_action(\n                        perception_result,\n                        self.current_task,\n                        self.current_pose\n                    )\n\n                    if next_action['type'] == 'navigate':\n                        self.navigation_system.set_goal(next_action['goal'])\n                        self.brain_state = 'NAVIGATING'\n                    elif next_action['type'] == 'execute':\n                        self.brain_state = 'EXECUTING'\n                        self.execute_action(next_action['action'])\n\n            elif self.brain_state == 'NAVIGATING':\n                # Execute navigation\n                nav_status = self.navigation_system.update(\n                    self.current_pose,\n                    self.current_twist\n                )\n\n                if nav_status == 'arrived':\n                    self.brain_state = 'EXECUTING'\n                elif nav_status == 'failed':\n                    self.brain_state = 'WAITING'  # Need human intervention\n\n            elif self.brain_state == 'EXECUTING':\n                # Execute the planned action\n                exec_status = self.execute_current_action()\n\n                if exec_status == 'completed':\n                    self.brain_state = 'IDLE'\n                    self.publish_behavior_status('Task completed')\n                elif exec_status == 'failed':\n                    self.brain_state = 'WAITING'\n\n            elif self.brain_state == 'WAITING':\n                # Wait for external command or timeout\n                pass\n\n    def execute_action(self, action):\n        \"\"\"Execute a specific action.\"\"\"\n        if action['type'] == 'pick_object':\n            return self.execute_pick_action(action)\n        elif action['type'] == 'place_object':\n            return self.execute_place_action(action)\n        elif action['type'] == 'inspect_area':\n            return self.execute_inspect_action(action)\n        else:\n            self.get_logger().warn(f'Unknown action type: {action[\"type\"]}')\n            return 'failed'\n\n    def execute_current_action(self):\n        \"\"\"Execute the current action.\"\"\"\n        # This would be implemented based on the specific action\n        # For now, return completed after some time\n        return 'completed'\n\n    def add_task(self, task):\n        \"\"\"Add a task to the queue.\"\"\"\n        self.task_queue.append(task)\n\n    def publish_behavior_status(self, status):\n        \"\"\"Publish behavior status.\"\"\"\n        status_msg = String()\n        status_msg.data = status\n        self.behavior_pub.publish(status_msg)\n\nclass PerceptionSystem:\n    \"\"\"Handles all perception tasks.\"\"\"\n    def __init__(self):\n        # Initialize perception models\n        self.object_detector = None  # Would be loaded model\n        self.segmentation_model = None  # Would be loaded model\n        self.depth_estimator = None  # Would be loaded model\n\n    def process(self, image_msg, scan_data, imu_data):\n        \"\"\"Process all sensor data for perception.\"\"\"\n        # Convert image\n        cv_image = CvBridge().imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\n\n        # Run object detection\n        objects = self.detect_objects(cv_image)\n\n        # Run semantic segmentation\n        segmentation = self.semantic_segment(cv_image)\n\n        # Process depth from stereo or structure from motion\n        depth_map = self.estimate_depth(cv_image)\n\n        # Fuse with laser and IMU data\n        environment_map = self.fuse_sensor_data(\n            objects, segmentation, depth_map, scan_data, imu_data\n        )\n\n        return {\n            'objects': objects,\n            'segmentation': segmentation,\n            'environment_map': environment_map,\n            'pose_estimate': self.estimate_pose(imu_data)\n        }\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in the image.\"\"\"\n        # Would use trained model\n        return []\n\n    def semantic_segment(self, image):\n        \"\"\"Perform semantic segmentation.\"\"\"\n        # Would use trained model\n        return []\n\n    def estimate_depth(self, image):\n        \"\"\"Estimate depth from image.\"\"\"\n        # Would use stereo or monocular depth estimation\n        return []\n\n    def fuse_sensor_data(self, objects, segmentation, depth_map, scan_data, imu_data):\n        \"\"\"Fuse all sensor data into coherent environment representation.\"\"\"\n        # Implementation would combine all modalities\n        return {}\n\n    def estimate_pose(self, imu_data):\n        \"\"\"Estimate pose from IMU data.\"\"\"\n        return {'position': [0, 0, 0], 'orientation': [0, 0, 0, 1]}\n\nclass NavigationSystem:\n    \"\"\"Handles navigation and path planning.\"\"\"\n    def __init__(self):\n        self.goal = None\n        self.current_path = []\n        self.local_planner = None  # DWA or other local planner\n        self.global_planner = None  # A* or other global planner\n        self.environment_map = {}\n\n    def set_goal(self, goal):\n        \"\"\"Set navigation goal.\"\"\"\n        self.goal = goal\n        self.plan_path()\n\n    def update_environment(self, perception_result):\n        \"\"\"Update environment map with new perception data.\"\"\"\n        self.environment_map = perception_result['environment_map']\n\n    def plan_path(self):\n        \"\"\"Plan path to goal.\"\"\"\n        if self.goal and self.environment_map:\n            # Call global planner\n            self.current_path = self.global_planner.plan(self.goal, self.environment_map)\n\n    def update(self, current_pose, current_twist):\n        \"\"\"Update navigation state.\"\"\"\n        if not self.current_path:\n            return 'arrived'\n\n        # Execute local planning and control\n        cmd_vel = self.local_planner.compute_velocity(\n            current_pose, current_twist, self.current_path\n        )\n\n        # Publish command\n        # Would publish to cmd_vel topic\n\n        # Check arrival\n        if self.is_at_goal(current_pose):\n            return 'arrived'\n\n        return 'navigating'\n\n    def is_at_goal(self, pose):\n        \"\"\"Check if robot is at goal.\"\"\"\n        # Implementation would check distance to goal\n        return False\n\nclass DecisionMaker:\n    \"\"\"Makes high-level decisions based on perception and tasks.\"\"\"\n    def __init__(self):\n        self.task_memory = {}  # Remember task outcomes\n        self.environment_model = {}  # Model of environment state\n\n    def decide_action(self, perception_result, current_task, current_pose):\n        \"\"\"Decide next action based on perception and task.\"\"\"\n        if current_task['type'] == 'explore':\n            return self.decide_explore_action(perception_result, current_task)\n        elif current_task['type'] == 'find_object':\n            return self.decide_find_object_action(perception_result, current_task)\n        elif current_task['type'] == 'navigate_to':\n            return self.decide_navigate_action(current_task)\n        else:\n            return {'type': 'wait', 'reason': 'unknown_task'}\n\n    def decide_explore_action(self, perception_result, task):\n        \"\"\"Decide action for exploration task.\"\"\"\n        # Find unexplored areas\n        unexplored = self.find_unexplored_areas(perception_result['environment_map'])\n        if unexplored:\n            return {'type': 'navigate', 'goal': unexplored[0]}\n        else:\n            return {'type': 'complete', 'reason': 'area_explored'}\n\n    def decide_find_object_action(self, perception_result, task):\n        \"\"\"Decide action for object finding task.\"\"\"\n        target_object = task['target_object']\n\n        # Check if object is in current view\n        for obj in perception_result['objects']:\n            if obj['class'] == target_object:\n                return {'type': 'navigate', 'goal': obj['location']}\n\n        # Otherwise, continue exploring\n        return self.decide_explore_action(perception_result, {'type': 'explore'})\n\n    def decide_navigate_action(self, task):\n        \"\"\"Decide action for navigation task.\"\"\"\n        return {'type': 'navigate', 'goal': task['destination']}\n\n    def find_unexplored_areas(self, env_map):\n        \"\"\"Find areas that haven't been explored.\"\"\"\n        # Implementation would analyze map for unknown areas\n        return []\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_brain = AIRobotBrain()\n\n    # Add example tasks\n    ai_brain.add_task({'type': 'explore', 'area': 'room1'})\n    ai_brain.add_task({'type': 'find_object', 'target_object': 'red_cube'})\n\n    try:\n        rclpy.spin(ai_brain)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_brain.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Create a launch file"})," (",(0,a.jsx)(n.code,{children:"launch/ai_robot_brain.launch.py"}),"):"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, RegisterEventHandler\nfrom launch.event_handlers import OnProcessExit\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    return LaunchDescription([\n        # Launch Arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n\n        # Launch Gazebo with robot\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                PathJoinSubstitution([\n                    FindPackageShare('gazebo_ros'),\n                    'launch',\n                    'empty_world.launch.py'\n                ])\n            ]),\n            launch_arguments={\n                'world': PathJoinSubstitution([FindPackageShare('your_package'), 'worlds', 'office_world.sdf']),\n                'paused': 'false',\n                'use_sim_time': use_sim_time,\n                'gui': 'true',\n                'headless': 'false',\n                'debug': 'false',\n            }.items()\n        ),\n\n        # Robot State Publisher\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            name='robot_state_publisher',\n            output='screen',\n            parameters=[{'use_sim_time': use_sim_time}],\n            arguments=[PathJoinSubstitution([FindPackageShare('your_package'), 'config', 'ai_robot.urdf'])]\n        ),\n\n        # Spawn Robot in Gazebo\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=[\n                '-entity', 'ai_robot',\n                '-file', PathJoinSubstitution([FindPackageShare('your_package'), 'config', 'ai_robot.urdf']),\n                '-x', '0',\n                '-y', '0',\n                '-z', '0.1'\n            ],\n            output='screen'\n        ),\n\n        # Launch AI Robot Brain\n        Node(\n            package='your_package',\n            executable='ai_robot_brain',\n            name='ai_robot_brain',\n            output='screen',\n            parameters=[{'use_sim_time': use_sim_time}]\n        ),\n\n        # Launch perception pipeline\n        Node(\n            package='your_package',\n            executable='perception_pipeline',\n            name='perception_pipeline',\n            output='screen',\n            parameters=[{'use_sim_time': use_sim_time}]\n        ),\n\n        # Launch navigation stack\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                PathJoinSubstitution([\n                    FindPackageShare('nav2_bringup'),\n                    'launch',\n                    'navigation_launch.py'\n                ])\n            ]),\n            launch_arguments={'use_sim_time': use_sim_time}.items(),\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"testing-the-project",children:"Testing the Project"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Launch the complete system: ",(0,a.jsx)(n.code,{children:"ros2 launch your_package ai_robot_brain.launch.py"})]}),"\n",(0,a.jsx)(n.li,{children:"Monitor the AI brain's decision making in RViz"}),"\n",(0,a.jsx)(n.li,{children:"Observe perception, navigation, and task execution"}),"\n",(0,a.jsx)(n.li,{children:"Add tasks dynamically using ROS 2 services"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"project-2-isaac-sim-perception-training-pipeline",children:"Project 2: Isaac Sim Perception Training Pipeline"}),"\n",(0,a.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,a.jsx)(n.p,{children:"Create a pipeline that uses Isaac Sim to generate synthetic data for training perception models."}),"\n",(0,a.jsx)(n.h3,{id:"requirements-1",children:"Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Generate diverse synthetic datasets"}),"\n",(0,a.jsx)(n.li,{children:"Include variations in lighting, textures, objects"}),"\n",(0,a.jsx)(n.li,{children:"Automatically annotate data"}),"\n",(0,a.jsx)(n.li,{children:"Train models using synthetic data"}),"\n",(0,a.jsx)(n.li,{children:"Validate on real data"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"project-3-multi-agent-ai-robot-coordination",children:"Project 3: Multi-Agent AI-Robot Coordination"}),"\n",(0,a.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,a.jsx)(n.p,{children:"Extend the AI-Robot brain to coordinate multiple robots for complex tasks."}),"\n",(0,a.jsx)(n.h3,{id:"requirements-2",children:"Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement multi-robot communication"}),"\n",(0,a.jsx)(n.li,{children:"Coordinate task allocation"}),"\n",(0,a.jsx)(n.li,{children:"Handle inter-robot collision avoidance"}),"\n",(0,a.jsx)(n.li,{children:"Demonstrate collaborative behaviors"}),"\n",(0,a.jsx)(n.li,{children:"Simulate in Isaac Sim environment"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"These projects helped you practice:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Creating complete AI-Robot brain systems"}),"\n",(0,a.jsx)(n.li,{children:"Integrating perception, navigation, and decision-making"}),"\n",(0,a.jsx)(n.li,{children:"Working with Isaac Sim for advanced robotics"}),"\n",(0,a.jsx)(n.li,{children:"Implementing complex autonomous behaviors"}),"\n",(0,a.jsx)(n.li,{children:"Building multi-component robotic systems"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Complete these projects to solidify your understanding of AI-Robot brains before moving to Module 4."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const a={},o=i.createContext(a);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);