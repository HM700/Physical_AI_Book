"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[433],{155(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/projects","title":"Module 4 Projects: Vision-Language-Action Integration","description":"In this section, you\'ll work on comprehensive projects that integrate all the concepts learned in the previous modules to create complete Vision-Language-Action systems.","source":"@site/docs/module4/projects.md","sourceDirName":"module4","slug":"/module4/projects","permalink":"/Physical-AI-Book/docs/module4/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/projects.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"VLA System Integration","permalink":"/Physical-AI-Book/docs/module4/integration"},"next":{"title":"Module 4 Conclusion","permalink":"/Physical-AI-Book/docs/module4/conclusion"}}');var o=t(4848),i=t(8453);const a={sidebar_position:4},r="Module 4 Projects: Vision-Language-Action Integration",l={},c=[{value:"Project 1: Complete VLA Assistant Robot",id:"project-1-complete-vla-assistant-robot",level:2},{value:"Objective",id:"objective",level:3},{value:"Requirements",id:"requirements",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Testing the Project",id:"testing-the-project",level:3},{value:"Project 2: Multi-Modal Learning Environment",id:"project-2-multi-modal-learning-environment",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Requirements",id:"requirements-1",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Project 3: VLA Research Platform",id:"project-3-vla-research-platform",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Requirements",id:"requirements-2",level:3},{value:"Implementation",id:"implementation-1",level:3},{value:"Best Practices for VLA Systems",id:"best-practices-for-vla-systems",level:2},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-projects-vision-language-action-integration",children:"Module 4 Projects: Vision-Language-Action Integration"})}),"\n",(0,o.jsx)(n.p,{children:"In this section, you'll work on comprehensive projects that integrate all the concepts learned in the previous modules to create complete Vision-Language-Action systems."}),"\n",(0,o.jsx)(n.h2,{id:"project-1-complete-vla-assistant-robot",children:"Project 1: Complete VLA Assistant Robot"}),"\n",(0,o.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete VLA system that can understand natural language commands, perceive its environment using vision, and execute appropriate actions in simulation."}),"\n",(0,o.jsx)(n.h3,{id:"requirements",children:"Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement speech recognition using Whisper for voice commands"}),"\n",(0,o.jsx)(n.li,{children:"Integrate LLM for natural language understanding and action mapping"}),"\n",(0,o.jsx)(n.li,{children:"Create perception system for object detection and scene understanding"}),"\n",(0,o.jsx)(n.li,{children:"Implement navigation and manipulation capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Build end-to-end pipeline from voice command to action execution"}),"\n",(0,o.jsx)(n.li,{children:"Ensure safety and error handling throughout"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create the main VLA node"})," (",(0,o.jsx)(n.code,{children:"src/vla_assistant.py"}),"):"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, Pose\nfrom audio_common_msgs.msg import AudioData\nfrom vision_msgs.msg import Detection2DArray\nimport speech_recognition as sr\nimport openai\nimport json\nimport asyncio\nfrom typing import Dict, Any, List, Optional\n\nclass VLAAssistantNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_assistant\')\n\n        # Initialize speech recognition\n        self.speech_recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.navigation_goal_pub = self.create_publisher(Pose, \'/move_base_simple/goal\', 10)\n        self.action_command_pub = self.create_publisher(String, \'/action_command\', 10)\n        self.tts_command_pub = self.create_publisher(String, \'/tts_command\', 10)\n        self.system_status_pub = self.create_publisher(String, \'/vla_system_status\', 10)\n\n        # Subscriptions\n        self.voice_command_sub = self.create_subscription(\n            AudioData, \'/audio_input\', self.voice_command_callback, 10\n        )\n        self.vision_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.vision_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, \'/detections\', self.detection_callback, 10\n        )\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10\n        )\n        self.action_feedback_sub = self.create_subscription(\n            String, \'/action_feedback\', self.action_feedback_callback, 10\n        )\n\n        # System state\n        self.current_pose = None\n        self.detected_objects = []\n        self.system_ready = False\n        self.active_command = None\n        self.command_history = []\n\n        # Semantic environment map\n        self.semantic_map = {\n            "locations": {\n                "kitchen": {"x": 2.0, "y": 1.0, "reachable": True},\n                "living_room": {"x": 0.0, "y": 0.0, "reachable": True},\n                "bedroom": {"x": -2.0, "y": 1.0, "reachable": True},\n                "office": {"x": 1.0, "y": -2.0, "reachable": True},\n                "corridor": {"x": 0.0, "y": -1.0, "reachable": True}\n            },\n            "objects": {\n                "cup": ["kitchen", "office"],\n                "book": ["bedroom", "office"],\n                "keys": ["bedroom", "living_room"],\n                "phone": ["office", "bedroom"],\n                "bottle": ["kitchen", "living_room"]\n            }\n        }\n\n        # Initialize system\n        self.initialize_system()\n\n        # Timers\n        self.processing_timer = self.create_timer(0.1, self.process_inputs)\n        self.status_timer = self.create_timer(1.0, self.publish_system_status)\n\n    def initialize_system(self):\n        """Initialize the VLA system."""\n        self.get_logger().info(\'Initializing VLA Assistant System...\')\n\n        # Calibrate speech recognition\n        with self.microphone as source:\n            self.speech_recognizer.adjust_for_ambient_noise(source)\n\n        # Set up semantic understanding\n        self.setup_semantic_understanding()\n\n        # Verify sensors are working\n        self.verify_sensor_setup()\n\n        self.system_ready = True\n        self.get_logger().info(\'VLA Assistant System initialized and ready!\')\n\n    def setup_semantic_understanding(self):\n        """Set up understanding of semantic environment."""\n        # In a real system, this would load semantic maps\n        self.get_logger().info(\'Semantic understanding initialized\')\n\n    def verify_sensor_setup(self):\n        """Verify all sensors are properly connected."""\n        # In a real system, this would verify sensor connections\n        self.get_logger().info(\'Sensor verification completed\')\n\n    def voice_command_callback(self, msg):\n        """Handle voice commands from speech recognition."""\n        try:\n            # Convert audio data to text using Whisper or similar\n            # In a real implementation, this would process the audio\n            voice_text = self.process_audio_to_text(msg)\n\n            self.get_logger().info(f\'Received voice command: {voice_text}\')\n\n            # Queue command for processing\n            command = {\n                \'id\': self.get_clock().now().nanoseconds,\n                \'original_text\': voice_text,\n                \'processed_text\': self.normalize_command(voice_text),\n                \'timestamp\': self.get_clock().now().nanoseconds,\n                \'status\': \'received\'\n            }\n\n            self.active_command = command\n            self.get_logger().info(f\'Processing command: {command["processed_text"]}\')\n\n            # Interpret command using LLM\n            interpreted_command = self.interpret_command_with_llm(voice_text)\n\n            # Execute the interpreted command\n            self.execute_interpreted_command(interpreted_command)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing voice command: {str(e)}\')\n            self.provide_error_feedback(str(e))\n\n    def process_audio_to_text(self, audio_msg):\n        """Process audio data to text (placeholder for Whisper integration)."""\n        # In a real implementation, this would use Whisper or similar\n        # For now, return a placeholder\n        return "Go to the kitchen and bring me a cup"\n\n    def normalize_command(self, command_text):\n        """Normalize command text for processing."""\n        normalized = command_text.lower().strip()\n\n        # Remove common filler words\n        fillers = [\'please\', \'could you\', \'can you\', \'would you\']\n        for filler in fillers:\n            normalized = normalized.replace(filler, \'\').strip()\n\n        return normalized\n\n    def interpret_command_with_llm(self, command_text):\n        """Use LLM to interpret natural language command."""\n        # In a real implementation, this would call an LLM API\n        # For now, we\'ll implement basic parsing\n\n        command_lower = command_text.lower()\n\n        if \'go to\' in command_lower or \'navigate to\' in command_lower or \'move to\' in command_lower:\n            # Extract location\n            for location in self.semantic_map[\'locations\'].keys():\n                if location in command_lower:\n                    return f"NAVIGATE_TO:{location.upper()}"\n\n        elif \'find\' in command_lower or \'look for\' in command_lower or \'where is\' in command_lower:\n            # Extract object\n            for obj in self.semantic_map[\'objects\'].keys():\n                if obj in command_lower:\n                    return f"FIND_OBJECT:{obj.upper()}"\n\n        elif \'bring\' in command_lower or \'get\' in command_lower or \'fetch\' in command_lower:\n            # Extract object and destination\n            obj_found = None\n            dest_found = None\n\n            for obj in self.semantic_map[\'objects\'].keys():\n                if obj in command_lower:\n                    obj_found = obj\n                    break\n\n            for loc in self.semantic_map[\'locations\'].keys():\n                if loc in command_lower:\n                    dest_found = loc\n                    break\n\n            if obj_found:\n                destination = dest_found or \'current_location\'  # Default to current location\n                return f"BRING_OBJECT:{obj_found.upper()}:{destination.upper()}"\n\n        elif \'follow\' in command_lower or \'follow me\' in command_lower:\n            return "FOLLOW_HUMAN:START"\n\n        elif \'stop\' in command_lower and \'follow\' in command_lower:\n            return "FOLLOW_HUMAN:STOP"\n\n        else:\n            # Use LLM for complex commands (simulated)\n            return self.simulate_llm_interpretation(command_text)\n\n    def simulate_llm_interpretation(self, command_text):\n        """Simulate LLM interpretation for complex commands."""\n        # This would be replaced with actual LLM call in production\n        self.get_logger().info(f\'Simulating LLM interpretation for: {command_text}\')\n\n        # Default interpretation\n        return f"UNKNOWN_COMMAND:{command_text[:50]}..."  # Truncate long commands\n\n    def execute_interpreted_command(self, interpreted_cmd):\n        """Execute an interpreted command."""\n        self.get_logger().info(f\'Executing interpreted command: {interpreted_cmd}\')\n\n        if interpreted_cmd.startswith(\'NAVIGATE_TO:\'):\n            location = interpreted_cmd.split(\':\')[1]\n            self.execute_navigation(location)\n\n        elif interpreted_cmd.startswith(\'FIND_OBJECT:\'):\n            obj_name = interpreted_cmd.split(\':\')[1]\n            self.execute_find_object(obj_name)\n\n        elif interpreted_cmd.startswith(\'BRING_OBJECT:\'):\n            parts = interpreted_cmd.split(\':\')\n            obj_name = parts[1]\n            dest_location = parts[2] if len(parts) > 2 else self.get_current_location()\n            self.execute_bring_object(obj_name, dest_location)\n\n        elif interpreted_cmd.startswith(\'FOLLOW_HUMAN:\'):\n            action = interpreted_cmd.split(\':\')[1]\n            self.execute_follow_human(action == \'START\')\n\n        else:\n            self.get_logger().warn(f\'Unknown interpreted command: {interpreted_cmd}\')\n            self.complete_active_command("failure")\n\n    def execute_navigation(self, location):\n        """Execute navigation to a location."""\n        if location.lower() in self.semantic_map["locations"]:\n            loc_data = self.semantic_map["locations"][location.lower()]\n\n            # Create navigation goal\n            goal_pose = Pose()\n            goal_pose.position.x = float(loc_data["x"])\n            goal_pose.position.y = float(loc_data["y"])\n            goal_pose.position.z = 0.0\n\n            # Set orientation to face forward\n            goal_pose.orientation.z = 0.0\n            goal_pose.orientation.w = 1.0\n\n            # Publish navigation goal\n            self.navigation_goal_pub.publish(goal_pose)\n\n            self.get_logger().info(f\'Navigating to {location} at ({loc_data["x"]}, {loc_data["y"]})\')\n        else:\n            self.get_logger().error(f\'Unknown location: {location}\')\n            self.complete_active_command("failure")\n\n    def execute_find_object(self, object_name):\n        """Execute object finding task."""\n        self.get_logger().info(f\'Finding object: {object_name}\')\n\n        # Check if object is known to be in current location\n        if object_name.lower() in self.semantic_map["objects"]:\n            known_locations = self.semantic_map["objects"][object_name.lower()]\n\n            if self.get_current_location().lower() in known_locations:\n                self.get_logger().info(f\'{object_name} should be in current location: {self.get_current_location()}\')\n\n                # Trigger perception to find the object\n                # In a real system, this would activate object detection\n                cmd_msg = String()\n                cmd_msg.data = f"FIND_LOCALIZED:{object_name}"\n                self.action_command_pub.publish(cmd_msg)\n            else:\n                # Navigate to known location first\n                target_loc = known_locations[0]  # Go to first known location\n                self.get_logger().info(f\'{object_name} should be in {target_loc}, navigating there first\')\n\n                # Set up callback for when navigation completes\n                self.pending_action = ("FIND_OBJECT", object_name)\n                self.execute_navigation(target_loc)\n        else:\n            # Object location unknown, search in current area\n            cmd_msg = String()\n            cmd_msg.data = f"SEARCH_FOR:{object_name}"\n            self.action_command_pub.publish(cmd_msg)\n\n    def execute_bring_object(self, object_name, destination):\n        """Execute bring object task."""\n        self.get_logger().info(f\'Bringing {object_name} to {destination}\')\n\n        # This is a complex task that requires:\n        # 1. Find the object\n        # 2. Navigate to it\n        # 3. Grasp it\n        # 4. Navigate to destination\n        # 5. Place it\n\n        # Set up multi-step task\n        self.multi_step_task = [\n            ("FIND_OBJECT", object_name),\n            ("NAVIGATE_TO_OBJECT", object_name),\n            ("GRASP_OBJECT", object_name),\n            ("NAVIGATE_TO", destination),\n            ("PLACE_OBJECT", object_name)\n        ]\n        self.current_task_step = 0\n\n        # Start first step\n        self.execute_task_step()\n\n    def execute_follow_human(self, start_following):\n        """Execute human following behavior."""\n        cmd_msg = String()\n        if start_following:\n            cmd_msg.data = "FOLLOW_HUMAN:START"\n            self.get_logger().info(\'Starting to follow human\')\n        else:\n            cmd_msg.data = "FOLLOW_HUMAN:STOP"\n            self.get_logger().info(\'Stopping human following\')\n\n        self.action_command_pub.publish(cmd_msg)\n\n    def execute_task_step(self):\n        """Execute the current step in a multi-step task."""\n        if hasattr(self, \'multi_step_task\') and self.current_task_step < len(self.multi_step_task):\n            action, target = self.multi_step_task[self.current_task_step]\n\n            if action == "FIND_OBJECT":\n                self.execute_find_object(target)\n            elif action == "NAVIGATE_TO_OBJECT":\n                # Navigate to where object was detected\n                # This would require object pose information\n                pass\n            elif action == "GRASP_OBJECT":\n                # Execute grasping action\n                cmd_msg = String()\n                cmd_msg.data = f"GRASP:{target}"\n                self.action_command_pub.publish(cmd_msg)\n            elif action == "NAVIGATE_TO":\n                self.execute_navigation(target)\n            elif action == "PLACE_OBJECT":\n                cmd_msg = String()\n                cmd_msg.data = f"PLACE:{target}"\n                self.action_command_pub.publish(cmd_msg)\n\n            self.current_task_step += 1\n\n    def vision_callback(self, msg):\n        """Process visual input."""\n        # In a real system, this would process camera images\n        # For now, we\'ll just log that we received an image\n        self.get_logger().debug(\'Received camera image\')\n\n    def detection_callback(self, msg):\n        """Process object detections."""\n        self.detected_objects = []\n        for detection in msg.detections:\n            if detection.results:\n                label = detection.results[0].hypothesis.class_id\n                confidence = detection.results[0].hypothesis.score\n                if confidence > 0.5:  # Confidence threshold\n                    self.detected_objects.append({\n                        \'label\': label,\n                        \'confidence\': confidence,\n                        \'bbox\': detection.bbox\n                    })\n\n        self.get_logger().info(f\'Detected {len(self.detected_objects)} objects\')\n\n    def laser_callback(self, msg):\n        """Process laser scan data for obstacle detection."""\n        # Check for obstacles in path\n        min_distance = min([r for r in msg.ranges if r > 0 and r < float(\'inf\')], default=float(\'inf\'))\n\n        if min_distance < 0.5:  # Too close to obstacle\n            self.get_logger().warn(f\'Obstacle detected at {min_distance:.2f}m, activating safety protocol\')\n            self.activate_safety_protocol("OBSTACLE_TOO_CLOSE")\n\n    def odom_callback(self, msg):\n        """Update robot pose from odometry."""\n        pose = msg.pose.pose\n        self.current_pose = {\n            \'x\': pose.position.x,\n            \'y\': pose.position.y,\n            \'theta\': self.quaternion_to_yaw(pose.orientation)\n        }\n\n    def action_feedback_callback(self, msg):\n        """Handle action execution feedback."""\n        feedback = msg.data\n        self.get_logger().info(f\'Action feedback: {feedback}\')\n\n        if feedback.startswith("SUCCESS:"):\n            if self.active_command:\n                self.complete_active_command("success")\n        elif feedback.startswith("FAILURE:") or feedback.startswith("ERROR:"):\n            if self.active_command:\n                self.complete_active_command("failure")\n\n    def complete_active_command(self, result):\n        """Complete the active command."""\n        if self.active_command:\n            self.get_logger().info(f\'Command completed with result: {result}\')\n\n            # Add to command history\n            self.command_history.append({\n                "command_id": self.active_command[\'id\'],\n                "original_text": self.active_command[\'original_text\'],\n                "result": result,\n                "timestamp": self.active_command[\'timestamp\']\n            })\n\n            # Clear active command\n            self.active_command = None\n\n            # Provide feedback to user\n            feedback_msg = String()\n            if result == "success":\n                feedback_msg.data = f"Successfully completed: {self.active_command[\'original_text\']}"\n            else:\n                feedback_msg.data = f"Failed to complete: {self.active_command[\'original_text\']}"\n\n            self.tts_command_pub.publish(feedback_msg)\n\n    def get_current_location(self):\n        """Get current location based on robot pose."""\n        if not self.current_pose:\n            return "unknown"\n\n        # Find closest location in semantic map\n        robot_x = self.current_pose[\'x\']\n        robot_y = self.current_pose[\'y\']\n\n        closest_location = None\n        min_distance = float(\'inf\')\n\n        for location, coords in self.semantic_map["locations"].items():\n            distance = ((robot_x - coords["x"])**2 + (robot_y - coords["y"])**2)**0.5\n            if distance < min_distance:\n                min_distance = distance\n                closest_location = location\n\n        return closest_location if min_distance < 2.0 else "unknown"  # Threshold of 2m\n\n    def quaternion_to_yaw(self, orientation):\n        """Convert quaternion to yaw angle."""\n        import math\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def process_inputs(self):\n        """Main processing loop."""\n        # This would handle continuous processing of inputs\n        # For now, just log status periodically\n        if self.system_ready and not self.active_command:\n            self.get_logger().debug(\'VLA system idle, waiting for commands\')\n\n    def publish_system_status(self):\n        """Publish system status."""\n        status = {\n            "system_ready": self.system_ready,\n            "current_location": self.get_current_location(),\n            "active_command": self.active_command[\'original_text\'] if self.active_command else None,\n            "detected_objects_count": len(self.detected_objects),\n            "robot_pose": self.current_pose,\n            "command_history_count": len(self.command_history),\n            "components": {\n                "speech_recognition": True,\n                "language_understanding": True,\n                "perception": True,\n                "navigation": True,\n                "manipulation": True  # Would be true if robot has manipulator\n            }\n        }\n\n        status_msg = String()\n        status_msg.data = json.dumps(status)\n        self.system_status_pub.publish(status_msg)\n\n    def provide_error_feedback(self, error_message):\n        """Provide error feedback to user."""\n        feedback_msg = String()\n        feedback_msg.data = f"Sorry, I encountered an error: {error_message}. Please try again."\n        self.tts_command_pub.publish(feedback_msg)\n\n    def activate_safety_protocol(self, reason):\n        """Activate safety protocol."""\n        self.get_logger().warn(f\'Safety protocol activated: {reason}\')\n\n        # Stop robot\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n        # Provide warning\n        warning_msg = String()\n        warning_msg.data = f"Safety alert: {reason.replace(\'_\', \' \').lower()}"\n        self.tts_command_pub.publish(warning_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_node = VLAAssistantNode()\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        vla_node.get_logger().info(\'VLA Assistant shutting down...\')\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create a launch file"})," (",(0,o.jsx)(n.code,{children:"launch/vla_assistant.launch.py"}),"):"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    return LaunchDescription([\n        # Launch Arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation clock if true'\n        ),\n\n        # Include Isaac Sim launch (if needed)\n        # IncludeLaunchDescription(\n        #     PythonLaunchDescriptionSource([\n        #         PathJoinSubstitution([\n        #             FindPackageShare('isaac_ros_bringup'),\n        #             'launch',\n        #             'isaac_sim.launch.py'\n        #         ])\n        #     ])\n        # ),\n\n        # VLA Assistant Node\n        Node(\n            package='physical_ai_book_nodes',\n            executable='vla_assistant',\n            name='vla_assistant',\n            output='screen',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ],\n            remappings=[\n                ('/audio_input', '/headless_cam/audio'),\n                ('/camera/rgb/image_raw', '/headless_cam/rgb'),\n                ('/detections', '/isaac_ros/detections'),\n                ('/cmd_vel', '/diff_drive_controller/cmd_vel_unstamped'),\n                ('/odom', '/diff_drive_controller/odom'),\n                ('/scan', '/scan'),\n                ('/tts_command', '/tts/input'),\n                ('/action_command', '/robot/action_command'),\n                ('/action_feedback', '/robot/action_feedback'),\n                ('/move_base_simple/goal', '/goal_pose')\n            ]\n        ),\n\n        # Perception pipeline (Isaac ROS nodes)\n        Node(\n            package='isaac_ros_detect_net',\n            executable='detectnet_node',\n            name='detectnet',\n            parameters=[\n                {'input_topic': '/camera/image_rect_color'},\n                {'network_type': 'ssd_mobilenet_v2'},\n                {'model_path': 'ssd_mobilenet_v2_coco.pt'},\n                {'class_labels_path': 'coco_labels.txt'}\n            ]\n        ),\n\n        # TTS Node for feedback\n        Node(\n            package='tts_ros2',\n            executable='tts_node',\n            name='tts_node',\n            parameters=[\n                {'voice': 'en-US-Wavenet-D'}\n            ]\n        )\n    ])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"testing-the-project",children:"Testing the Project"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Launch the simulation environment"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Launch Isaac Sim environment\nros2 launch isaac_ros_bringup isaac_sim.launch.py\n\n# In another terminal, launch the VLA assistant\nros2 launch physical_ai_book_nodes vla_assistant.launch.py\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Test different commands"}),":"]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Go to the kitchen"'}),"\n",(0,o.jsx)(n.li,{children:'"Find my keys"'}),"\n",(0,o.jsx)(n.li,{children:'"Bring me a cup from the kitchen"'}),"\n",(0,o.jsx)(n.li,{children:'"Follow me"'}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Monitor the system"}),":"]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Check the command processing in the terminal"}),"\n",(0,o.jsx)(n.li,{children:"Monitor RViz for navigation behavior"}),"\n",(0,o.jsx)(n.li,{children:"Verify object detection is working"}),"\n",(0,o.jsx)(n.li,{children:"Test safety protocols"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"project-2-multi-modal-learning-environment",children:"Project 2: Multi-Modal Learning Environment"}),"\n",(0,o.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Create a learning environment that integrates visual, auditory, and textual modalities for enhanced Physical AI education."}),"\n",(0,o.jsx)(n.h3,{id:"requirements-1",children:"Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement multi-modal input processing (voice, text, visual)"}),"\n",(0,o.jsx)(n.li,{children:"Create adaptive learning pathways based on user interaction"}),"\n",(0,o.jsx)(n.li,{children:"Integrate with Isaac Sim for realistic simulation feedback"}),"\n",(0,o.jsx)(n.li,{children:"Provide real-time performance assessment"}),"\n",(0,o.jsx)(n.li,{children:"Include gamification elements"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Modal Input Handler"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class MultiModalInputHandler(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_handler\')\n\n        # Subscriptions for multiple input modalities\n        self.text_command_sub = self.create_subscription(\n            String, \'/text_commands\', self.text_callback, 10\n        )\n        self.voice_command_sub = self.create_subscription(\n            String, \'/voice_commands\', self.voice_callback, 10\n        )\n        self.gesture_sub = self.create_subscription(\n            Gesture, \'/gestures\', self.gesture_callback, 10\n        )\n\n        # Publishers\n        self.response_pub = self.create_publisher(String, \'/multimodal_response\', 10)\n        self.learning_path_pub = self.create_publisher(LearningPath, \'/learning_path\', 10)\n\n        # Learning state\n        self.user_progress = {}\n        self.adaptive_paths = {}\n\n    def text_callback(self, msg):\n        """Handle text-based commands."""\n        self.process_multimodal_input(\'text\', msg.data)\n\n    def voice_callback(self, msg):\n        """Handle voice-based commands."""\n        self.process_multimodal_input(\'voice\', msg.data)\n\n    def gesture_callback(self, msg):\n        """Handle gesture-based commands."""\n        self.process_multimodal_input(\'gesture\', msg)\n\n    def process_multimodal_input(self, input_type, input_data):\n        """Process input from any modality."""\n        # Fuse inputs and create unified understanding\n        fused_input = self.fuse_modalities(input_type, input_data)\n\n        # Generate appropriate response based on learning objectives\n        response = self.generate_learning_response(fused_input)\n\n        # Update user progress and learning path\n        self.update_learning_path(response)\n\n        # Publish response\n        response_msg = String()\n        response_msg.data = response\n        self.response_pub.publish(response_msg)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"project-3-vla-research-platform",children:"Project 3: VLA Research Platform"}),"\n",(0,o.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"Create a research platform for experimenting with Vision-Language-Action systems, including data collection, analysis, and experimentation tools."}),"\n",(0,o.jsx)(n.h3,{id:"requirements-2",children:"Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement data logging and collection system"}),"\n",(0,o.jsx)(n.li,{children:"Create experiment management interface"}),"\n",(0,o.jsx)(n.li,{children:"Add performance analysis tools"}),"\n",(0,o.jsx)(n.li,{children:"Include A/B testing capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Provide visualization and reporting"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data Collection System"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class VLADataCollector(Node):\n    def __init__(self):\n        super().__init__('vla_data_collector')\n\n        # Subscriptions for all relevant data streams\n        self.command_sub = self.create_subscription(\n            String, '/commands', self.command_callback, 10\n        )\n        self.vision_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.vision_callback, 10\n        )\n        self.actions_sub = self.create_subscription(\n            String, '/actions', self.action_callback, 10\n        )\n        self.performance_sub = self.create_subscription(\n            PerformanceMetrics, '/performance', self.performance_callback, 10\n        )\n\n        # Data storage\n        self.experiment_data = []\n        self.data_buffer = []\n\n        # Timers\n        self.flush_timer = self.create_timer(1.0, self.flush_buffer)\n\n    def command_callback(self, msg):\n        \"\"\"Log command data.\"\"\"\n        self.log_data('command', {\n            'timestamp': self.get_clock().now().nanoseconds,\n            'command': msg.data,\n            'type': 'natural_language'\n        })\n\n    def vision_callback(self, msg):\n        \"\"\"Log vision data.\"\"\"\n        self.log_data('vision', {\n            'timestamp': msg.header.stamp.sec * 1e9 + msg.header.stamp.nanosec,\n            'encoding': msg.encoding,\n            'height': msg.height,\n            'width': msg.width\n        })\n\n    def log_data(self, data_type, data):\n        \"\"\"Log data to buffer.\"\"\"\n        entry = {\n            'type': data_type,\n            'data': data,\n            'logged_at': self.get_clock().now().nanoseconds\n        }\n        self.data_buffer.append(entry)\n\n    def flush_buffer(self):\n        \"\"\"Flush data buffer to persistent storage.\"\"\"\n        if self.data_buffer:\n            # In a real system, this would write to database or file\n            self.experiment_data.extend(self.data_buffer)\n            self.data_buffer.clear()\n            self.get_logger().info(f'Flushed {len(self.experiment_data)} total data entries')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-vla-systems",children:"Best Practices for VLA Systems"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robust Error Handling"}),": Always have fallback behaviors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety First"}),": Implement comprehensive safety checks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Performance"}),": Optimize for real-time constraints"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"User Feedback"}),": Provide clear feedback for all actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Privacy Protection"}),": Secure personal data and conversations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Continuous Learning"}),": Adapt to user preferences over time"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete VLA system that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Integrates all four modules (ROS 2, Digital Twin, AI-Robot Brain, VLA)"}),"\n",(0,o.jsx)(n.li,{children:"Implements the full voice-to-action pipeline"}),"\n",(0,o.jsx)(n.li,{children:"Includes comprehensive safety measures"}),"\n",(0,o.jsx)(n.li,{children:"Provides adaptive learning capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Logs and analyzes performance data"}),"\n",(0,o.jsx)(n.li,{children:"Demonstrates complex multi-step tasks"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this module, you've learned to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Create complete Vision-Language-Action systems"}),"\n",(0,o.jsx)(n.li,{children:"Integrate perception, language understanding, and action execution"}),"\n",(0,o.jsx)(n.li,{children:"Implement robust safety and error handling"}),"\n",(0,o.jsx)(n.li,{children:"Build adaptive learning environments"}),"\n",(0,o.jsx)(n.li,{children:"Create research platforms for VLA experimentation"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These projects provide hands-on experience with the complete Physical AI pipeline from basic ROS 2 concepts to advanced VLA systems. You now have the knowledge and skills to build sophisticated Physical AI systems that can perceive, reason, and act in the physical world."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);