"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[433],{155(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/projects","title":"Module 4 Projects","description":"In this section, you\'ll work on comprehensive projects that integrate all four modules to create complete Physical AI systems capable of understanding natural language commands, perceiving their environment, and executing complex tasks.","source":"@site/docs/module4/projects.md","sourceDirName":"module4","slug":"/module4/projects","permalink":"/Physical-AI-Book/docs/module4/projects","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/projects.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"VLA System Integration","permalink":"/Physical-AI-Book/docs/module4/integration"},"next":{"title":"Module 4 Conclusion","permalink":"/Physical-AI-Book/docs/module4/conclusion"}}');var i=t(4848),a=t(8453);const o={sidebar_position:4},r="Module 4 Projects",l={},c=[{value:"Project 1: Complete VLA Home Assistant Robot",id:"project-1-complete-vla-home-assistant-robot",level:2},{value:"Objective",id:"objective",level:3},{value:"Requirements",id:"requirements",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Testing the Project",id:"testing-the-project",level:3},{value:"Project 2: Multi-Room House Mapping and Monitoring",id:"project-2-multi-room-house-mapping-and-monitoring",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Requirements",id:"requirements-1",level:3},{value:"Project 3: Adaptive Learning Companion Robot",id:"project-3-adaptive-learning-companion-robot",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Requirements",id:"requirements-2",level:3},{value:"Capstone Challenge: Full Home Automation Integration",id:"capstone-challenge-full-home-automation-integration",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Requirements",id:"requirements-3",level:3},{value:"Implementation Strategy",id:"implementation-strategy",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"module-4-projects",children:"Module 4 Projects"})}),"\n",(0,i.jsx)(n.p,{children:"In this section, you'll work on comprehensive projects that integrate all four modules to create complete Physical AI systems capable of understanding natural language commands, perceiving their environment, and executing complex tasks."}),"\n",(0,i.jsx)(n.h2,{id:"project-1-complete-vla-home-assistant-robot",children:"Project 1: Complete VLA Home Assistant Robot"}),"\n",(0,i.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete VLA system that acts as a home assistant robot, responding to voice commands to perform household tasks."}),"\n",(0,i.jsx)(n.h3,{id:"requirements",children:"Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement voice command recognition using Whisper"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with LLM for natural language understanding"}),"\n",(0,i.jsx)(n.li,{children:"Use perception system to identify objects and navigate"}),"\n",(0,i.jsx)(n.li,{children:"Execute complex multi-step tasks safely"}),"\n",(0,i.jsx)(n.li,{children:"Provide feedback to users through text-to-speech"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with all previous modules (ROS 2, Digital Twin, AI-Robot Brain)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create the system architecture"})," (",(0,i.jsx)(n.code,{children:"launch/home_assistant.launch.py"}),"):"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, GroupAction\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    return LaunchDescription([\n        # Launch Arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation clock if true'\n        ),\n\n        # Launch Isaac Sim environment\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                PathJoinSubstitution([\n                    FindPackageShare('isaac_sim'),\n                    'launch',\n                    'standalone.launch.py'\n                ])\n            ]),\n            launch_arguments={\n                'headless': 'false',\n                'enable_ros': 'true'\n            }.items()\n        ),\n\n        # Launch robot in simulation\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=[\n                '-entity', 'home_assistant_robot',\n                '-file', PathJoinSubstitution([FindPackageShare('home_assistant_pkg'), 'models', 'assistant_robot.urdf']),\n                '-x', '0', '-y', '0', '-z', '0.1'\n            ],\n            output='screen'\n        ),\n\n        # Launch speech recognition\n        Node(\n            package='home_assistant_pkg',\n            executable='whisper_stt',\n            name='speech_to_text',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Launch LLM interface\n        Node(\n            package='home_assistant_pkg',\n            executable='llm_interface',\n            name='llm_interface',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Launch cognitive planner\n        Node(\n            package='home_assistant_pkg',\n            executable='cognitive_planner',\n            name='cognitive_planner',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Launch perception pipeline\n        Node(\n            package='home_assistant_pkg',\n            executable='perception_pipeline',\n            name='perception_pipeline',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Launch navigation stack\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource([\n                PathJoinSubstitution([\n                    FindPackageShare('nav2_bringup'),\n                    'launch',\n                    'navigation_launch.py'\n                ])\n            ]),\n            launch_arguments={\n                'use_sim_time': use_sim_time,\n                'params_file': PathJoinSubstitution([FindPackageShare('home_assistant_pkg'), 'config', 'nav2_params.yaml'])\n            }.items(),\n        ),\n\n        # Launch VLA integration\n        Node(\n            package='home_assistant_pkg',\n            executable='vla_integration',\n            name='vla_integration',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        ),\n\n        # Launch safety manager\n        Node(\n            package='home_assistant_pkg',\n            executable='safety_manager',\n            name='safety_manager',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create the main VLA integration node"})," (extending from previous example):"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Pose, Twist\nfrom audio_common_msgs.msg import AudioData\nfrom vision_msgs.msg import Detection2DArray\nfrom nav_msgs.msg import Odometry\nimport json\nimport threading\nimport queue\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any, List\n\nclass TaskState(Enum):\n    IDLE = "idle"\n    LISTENING = "listening"\n    PROCESSING = "processing"\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    SAFETY_STOP = "safety_stop"\n\n@dataclass\nclass HouseholdTask:\n    id: str\n    type: str  # \'fetch_item\', \'navigation\', \'inspection\', \'monitoring\'\n    target: str  # object or location\n    destination: Optional[str] = None\n    priority: int = 1\n    created_at: Optional[float] = None\n\nclass HomeAssistantVLANode(Node):\n    def __init__(self):\n        super().__init__(\'home_assistant_vla\')\n\n        # Initialize system components\n        self.initialize_system()\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.navigation_goal_pub = self.create_publisher(Pose, \'/move_base_simple/goal\', 10)\n        self.action_command_pub = self.create_publisher(String, \'/robot/action_command\', 10)\n        self.tts_pub = self.create_publisher(String, \'/tts_command\', 10)\n        self.system_status_pub = self.create_publisher(String, \'/system_status\', 10)\n\n        # Subscriptions\n        self.voice_sub = self.create_subscription(\n            String, \'/speech_text\', self.voice_command_callback, 10\n        )\n        self.interpreted_sub = self.create_subscription(\n            String, \'/interpreted_command\', self.interpreted_command_callback, 10\n        )\n        self.perception_sub = self.create_subscription(\n            Detection2DArray, \'/detections\', self.perception_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        self.action_feedback_sub = self.create_subscription(\n            String, \'/action_feedback\', self.action_feedback_callback, 10\n        )\n\n        # System state\n        self.current_state = TaskState.IDLE\n        self.active_task: Optional[HouseholdTask] = None\n        self.task_queue = queue.Queue()\n        self.robot_pose = {"x": 0.0, "y": 0.0, "theta": 0.0}\n        self.detected_objects = []\n        self.safe_zones = ["living_room", "kitchen", "hallway"]  # Safe areas for robot\n        self.dangerous_zones = ["stairs", "outlet", "hot_surface"]  # Dangerous areas\n\n        # Household map\n        self.household_map = {\n            "rooms": {\n                "living_room": {"x": 0.0, "y": 0.0, "accessible": True},\n                "kitchen": {"x": 2.0, "y": 1.0, "accessible": True},\n                "bedroom": {"x": -2.0, "y": 1.0, "accessible": True},\n                "bathroom": {"x": -1.0, "y": -2.0, "accessible": True},\n                "office": {"x": 1.0, "y": -2.0, "accessible": True}\n            },\n            "objects": {\n                "keys": ["bedroom", "living_room"],\n                "phone": ["office", "bedroom"],\n                "water": ["kitchen"],\n                "coffee": ["kitchen"],\n                "book": ["office", "bedroom"],\n                "medicine": ["bedroom"],\n                "snack": ["kitchen"]\n            },\n            "charging_station": {"x": 0.0, "y": -3.0}\n        }\n\n        # User preferences\n        self.user_preferences = {\n            "voice_volume": 0.7,\n            "operation_hours": {"start": 8, "end": 22},  # 24-hour format\n            "preferred_locations": ["kitchen", "living_room"],\n            "avoid_locations": ["bathroom"]\n        }\n\n        # Timers\n        self.main_loop_timer = self.create_timer(0.1, self.main_loop)\n        self.status_timer = self.create_timer(1.0, self.publish_system_status)\n\n        # Initialize\n        self.get_logger().info(\'Home Assistant VLA System initialized\')\n\n    def initialize_system(self):\n        """Initialize the complete system."""\n        self.get_logger().info(\'Initializing Home Assistant VLA System...\')\n\n        # Initialize all components\n        self.initialize_perception()\n        self.initialize_navigation()\n        self.initialize_language_processing()\n        self.initialize_safety_system()\n\n        self.get_logger().info(\'Home Assistant VLA System ready!\')\n\n    def initialize_perception(self):\n        """Initialize perception system."""\n        # Load object detection models\n        # Configure cameras and sensors\n        pass\n\n    def initialize_navigation(self):\n        """Initialize navigation system."""\n        # Load maps\n        # Configure path planner\n        # Set up obstacle detection\n        pass\n\n    def initialize_language_processing(self):\n        """Initialize language processing system."""\n        # Configure Whisper\n        # Set up LLM connection\n        # Load command vocabulary\n        pass\n\n    def initialize_safety_system(self):\n        """Initialize safety system."""\n        # Set up safety zones\n        # Configure emergency stops\n        # Load safety protocols\n        pass\n\n    def voice_command_callback(self, msg):\n        """Handle voice commands from user."""\n        command_text = msg.data.strip()\n        self.get_logger().info(f\'Received voice command: "{command_text}"\')\n\n        # Validate command\n        if self.is_command_allowed(command_text):\n            # Create task\n            task = HouseholdTask(\n                id=f"task_{self.get_clock().now().nanoseconds}",\n                type="command",\n                target=command_text\n            )\n\n            # Add to queue\n            self.task_queue.put(task)\n            self.get_logger().info(\'Command added to task queue\')\n        else:\n            self.provide_feedback("Command not allowed during current conditions")\n\n    def interpreted_command_callback(self, msg):\n        """Handle interpreted commands from LLM."""\n        interpreted_cmd = msg.data\n        self.get_logger().info(f\'Interpreted command: {interpreted_cmd}\')\n\n        if interpreted_cmd.startswith(\'FETCH_ITEM:\'):\n            item = interpreted_cmd.split(\':\')[1]\n            self.create_fetch_task(item)\n        elif interpreted_cmd.startswith(\'NAVIGATE_TO:\'):\n            location = interpreted_cmd.split(\':\')[1]\n            self.create_navigation_task(location)\n        elif interpreted_cmd.startswith(\'FIND_ITEM:\'):\n            item = interpreted_cmd.split(\':\')[1]\n            self.create_find_task(item)\n        elif interpreted_cmd.startswith(\'BRING_ITEM:\'):\n            parts = interpreted_cmd.split(\':\')\n            item = parts[1]\n            destination = parts[2] if len(parts) > 2 else "current_location"\n            self.create_bring_task(item, destination)\n        elif interpreted_cmd.startswith(\'MONITOR_AREA:\'):\n            area = interpreted_cmd.split(\':\')[1]\n            self.create_monitoring_task(area)\n\n    def perception_callback(self, msg):\n        """Handle perception data."""\n        self.detected_objects = []\n        for detection in msg.detections:\n            if detection.results:\n                for result in detection.results:\n                    if result.hypothesis.score > 0.5:  # Confidence threshold\n                        self.detected_objects.append({\n                            \'label\': result.hypothesis.class_id,\n                            \'confidence\': result.hypothesis.score,\n                            \'position\': detection.bbox.center\n                        })\n\n    def odom_callback(self, msg):\n        """Update robot pose."""\n        pose = msg.pose.pose\n        self.robot_pose["x"] = pose.position.x\n        self.robot_pose["y"] = pose.position.y\n\n        # Convert orientation to angle\n        from tf_transformations import euler_from_quaternion\n        import math\n        (_, _, theta) = euler_from_quaternion([\n            pose.orientation.x,\n            pose.orientation.y,\n            pose.orientation.z,\n            pose.orientation.w\n        ])\n        self.robot_pose["theta"] = theta\n\n    def scan_callback(self, msg):\n        """Handle laser scan for obstacle detection."""\n        # Check for obstacles in path\n        min_distance = min([r for r in msg.ranges if r > 0 and r < float(\'inf\')], default=float(\'inf\'))\n\n        if min_distance < 0.5:  # Too close to obstacle\n            self.activate_safety_protocol("OBSTACLE_TOO_CLOSE")\n\n    def action_feedback_callback(self, msg):\n        """Handle action execution feedback."""\n        feedback = msg.data\n        self.get_logger().info(f\'Action feedback: {feedback}\')\n\n        if feedback.startswith("SUCCESS:"):\n            if self.active_task:\n                self.complete_task("success")\n        elif feedback.startswith("FAILURE:") or feedback.startswith("ERROR:"):\n            if self.active_task:\n                self.complete_task("failure")\n\n    def main_loop(self):\n        """Main system loop."""\n        if self.current_state == TaskState.IDLE and not self.task_queue.empty():\n            self.start_next_task()\n        elif self.current_state == TaskState.EXECUTING:\n            self.continue_task_execution()\n        elif self.current_state == TaskState.SAFETY_STOP:\n            self.handle_safety_stop()\n\n    def start_next_task(self):\n        """Start the next task in the queue."""\n        if not self.task_queue.empty():\n            try:\n                task = self.task_queue.get_nowait()\n                self.active_task = task\n                self.current_state = TaskState.PROCESSING\n\n                self.get_logger().info(f\'Starting task: {task.type} - {task.target}\')\n\n                # Process the task\n                self.execute_household_task(task)\n\n            except queue.Empty:\n                pass\n\n    def execute_household_task(self, task):\n        """Execute a household task."""\n        if task.type == "fetch_item":\n            self.execute_fetch_task(task)\n        elif task.type == "navigation":\n            self.execute_navigation_task(task)\n        elif task.type == "find_item":\n            self.execute_find_task(task)\n        elif task.type == "bring_item":\n            self.execute_bring_task(task)\n        elif task.type == "monitor_area":\n            self.execute_monitoring_task(task)\n        else:\n            self.get_logger().warn(f\'Unknown task type: {task.type}\')\n            self.complete_task("failure")\n\n    def execute_fetch_task(self, task):\n        """Execute fetch item task."""\n        item = task.target\n        self.get_logger().info(f\'Fetching item: {item}\')\n\n        # Find item location\n        item_location = self.find_item_location(item)\n        if item_location:\n            # Navigate to item location\n            self.navigate_to_location(item_location)\n\n            # Wait for navigation to complete, then grasp\n            # This would be handled by a state machine\n            self.current_state = TaskState.EXECUTING\n        else:\n            self.get_logger().warn(f\'Item {item} location unknown\')\n            self.provide_feedback(f"Sorry, I don\'t know where the {item} is located")\n            self.complete_task("failure")\n\n    def execute_navigation_task(self, task):\n        """Execute navigation task."""\n        location = task.target\n        if location in self.household_map["rooms"]:\n            self.navigate_to_location(location)\n            self.current_state = TaskState.EXECUTING\n        else:\n            self.get_logger().warn(f\'Unknown location: {location}\')\n            self.provide_feedback(f"Sorry, I don\'t know where {location} is")\n            self.complete_task("failure")\n\n    def execute_find_task(self, task):\n        """Execute find item task."""\n        item = task.target\n        self.get_logger().info(f\'Finding item: {item}\')\n\n        # Search for item in known locations\n        if item in self.household_map["objects"]:\n            known_locations = self.household_map["objects"][item]\n            for location in known_locations:\n                if self.is_location_accessible(location):\n                    self.navigate_to_location(location)\n\n                    # Look for item\n                    # This would trigger perception system\n                    break\n        else:\n            # Search in common areas\n            common_areas = ["kitchen", "living_room", "office"]\n            for area in common_areas:\n                self.navigate_to_location(area)\n                # Look for item\n                break\n\n        self.current_state = TaskState.EXECUTING\n\n    def execute_bring_task(self, task):\n        """Execute bring item task."""\n        item = task.target\n        destination = task.destination or "current_location"\n\n        self.get_logger().info(f\'Bringing {item} to {destination}\')\n\n        # Complex task: fetch + navigate + deliver\n        self.multi_step_task = [\n            ("find", item),\n            ("fetch", item),\n            ("navigate", destination),\n            ("deliver", item)\n        ]\n        self.current_task_step = 0\n        self.execute_task_step()\n\n    def execute_task_step(self):\n        """Execute the current step in a multi-step task."""\n        if hasattr(self, \'multi_step_task\') and self.current_task_step < len(self.multi_step_task):\n            action, target = self.multi_step_task[self.current_task_step]\n\n            if action == "find":\n                self.execute_find_task(HouseholdTask("temp", "find_item", target))\n            elif action == "fetch":\n                self.execute_fetch_task(HouseholdTask("temp", "fetch_item", target))\n            elif action == "navigate":\n                self.execute_navigation_task(HouseholdTask("temp", "navigation", target))\n            elif action == "deliver":\n                # Place item at current location\n                cmd_msg = String()\n                cmd_msg.data = f"PLACE:{target}"\n                self.action_command_pub.publish(cmd_msg)\n\n            self.current_task_step += 1\n\n    def continue_task_execution(self):\n        """Continue executing the current task."""\n        # This would be handled by the underlying systems\n        # Check if task is complete based on feedback\n        pass\n\n    def complete_task(self, result):\n        """Complete the active task."""\n        if self.active_task:\n            self.get_logger().info(f\'Task completed with result: {result}\')\n\n            # Provide feedback\n            if result == "success":\n                if self.active_task.type == "fetch_item":\n                    self.provide_feedback(f"I successfully fetched the {self.active_task.target}")\n                elif self.active_task.type == "navigation":\n                    self.provide_feedback(f"I have arrived at {self.active_task.target}")\n                elif self.active_task.type == "bring_item":\n                    self.provide_feedback(f"I brought the {self.active_task.target} to the requested location")\n            else:\n                self.provide_feedback(f"I couldn\'t complete the task: {self.active_task.target}")\n\n            # Reset for next task\n            self.active_task = None\n            self.current_state = TaskState.IDLE\n\n    def find_item_location(self, item):\n        """Find the most likely location of an item."""\n        if item in self.household_map["objects"]:\n            # For now, return the first known location\n            # In a real system, this would use probability and recent observations\n            return self.household_map["objects"][item][0]\n        else:\n            return None\n\n    def navigate_to_location(self, location):\n        """Navigate to a specific location."""\n        if location in self.household_map["rooms"]:\n            room_data = self.household_map["rooms"][location]\n\n            goal_pose = Pose()\n            goal_pose.position.x = room_data["x"]\n            goal_pose.position.y = room_data["y"]\n            goal_pose.position.z = 0.0\n\n            # Set orientation\n            goal_pose.orientation.z = 0.0\n            goal_pose.orientation.w = 1.0\n\n            self.navigation_goal_pub.publish(goal_pose)\n            self.get_logger().info(f\'Navigating to {location} at ({room_data["x"]}, {room_data["y"]})\')\n        else:\n            self.get_logger().warn(f\'Unknown location: {location}\')\n\n    def is_location_accessible(self, location):\n        """Check if a location is accessible."""\n        return (location in self.household_map["rooms"] and\n                self.household_map["rooms"][location]["accessible"])\n\n    def is_command_allowed(self, command):\n        """Check if a command is allowed in current context."""\n        # Check if within operating hours\n        import datetime\n        current_hour = datetime.datetime.now().hour\n        if not (self.user_preferences["operation_hours"]["start"] <=\n                current_hour <=\n                self.user_preferences["operation_hours"]["end"]):\n            return False\n\n        # Check if location is allowed\n        # Additional safety checks would go here\n\n        return True\n\n    def provide_feedback(self, message):\n        """Provide feedback to user."""\n        feedback_msg = String()\n        feedback_msg.data = message\n        self.tts_pub.publish(feedback_msg)\n        self.get_logger().info(f\'Providing feedback: {message}\')\n\n    def activate_safety_protocol(self, reason):\n        """Activate safety protocol."""\n        self.get_logger().warn(f\'Safety protocol activated: {reason}\')\n        self.current_state = TaskState.SAFETY_STOP\n\n        # Stop robot\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n        # Provide warning\n        self.provide_feedback(f"Safety alert: {reason.replace(\'_\', \' \').lower()}")\n\n    def handle_safety_stop(self):\n        """Handle safety stop state."""\n        # Wait for manual override or automatic recovery\n        pass\n\n    def publish_system_status(self):\n        """Publish system status."""\n        status = {\n            "state": self.current_state.value,\n            "active_task": self.active_task.target if self.active_task else None,\n            "robot_position": self.robot_pose,\n            "detected_objects_count": len(self.detected_objects),\n            "task_queue_size": self.task_queue.qsize(),\n            "charging_needed": self.should_charge()\n        }\n\n        status_msg = String()\n        status_msg.data = json.dumps(status)\n        self.system_status_pub.publish(status_msg)\n\n    def should_charge(self):\n        """Determine if robot needs charging."""\n        # Simplified: check distance to charging station\n        charge_station = self.household_map["charging_station"]\n        dist_to_charger = ((self.robot_pose["x"] - charge_station["x"])**2 +\n                          (self.robot_pose["y"] - charge_station["y"])**2)**0.5\n\n        return dist_to_charger > 5.0  # Needs charging if far from station\n\ndef main(args=None):\n    rclpy.init(args=args)\n    home_assistant = HomeAssistantVLANode()\n\n    try:\n        rclpy.spin(home_assistant)\n    except KeyboardInterrupt:\n        home_assistant.get_logger().info(\'Shutting down Home Assistant VLA System\')\n    finally:\n        home_assistant.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create a task management system"}),":"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class TaskScheduler:\n    """Manages scheduling and prioritization of tasks."""\n\n    def __init__(self, vla_system):\n        self.vla_system = vla_system\n        self.task_queue = queue.PriorityQueue()\n        self.running_tasks = []\n        self.completed_tasks = []\n\n    def add_task(self, task, priority=1):\n        """Add a task to the scheduler."""\n        # Priority: lower number = higher priority\n        self.task_queue.put((priority, task))\n\n    def schedule_tasks(self):\n        """Schedule tasks based on priority and dependencies."""\n        # Implement scheduling algorithm\n        pass\n\n    def get_next_task(self):\n        """Get the next highest priority task."""\n        try:\n            return self.task_queue.get_nowait()[1]  # Return just the task, not priority\n        except queue.Empty:\n            return None\n'})}),"\n",(0,i.jsx)(n.h3,{id:"testing-the-project",children:"Testing the Project"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Launch the complete system: ",(0,i.jsx)(n.code,{children:"ros2 launch home_assistant_pkg home_assistant.launch.py"})]}),"\n",(0,i.jsx)(n.li,{children:'Give voice commands like "Bring me my keys" or "Go to the kitchen"'}),"\n",(0,i.jsx)(n.li,{children:"Observe the robot's behavior in Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"Monitor the system status and task execution"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-2-multi-room-house-mapping-and-monitoring",children:"Project 2: Multi-Room House Mapping and Monitoring"}),"\n",(0,i.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a system that can map a multi-room house and monitor it for security purposes."}),"\n",(0,i.jsx)(n.h3,{id:"requirements-1",children:"Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Autonomous mapping of house layout"}),"\n",(0,i.jsx)(n.li,{children:"Object detection and tracking"}),"\n",(0,i.jsx)(n.li,{children:"Anomaly detection in environment"}),"\n",(0,i.jsx)(n.li,{children:"Security alerts and notifications"}),"\n",(0,i.jsx)(n.li,{children:"Integration with home automation systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-3-adaptive-learning-companion-robot",children:"Project 3: Adaptive Learning Companion Robot"}),"\n",(0,i.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a robot that learns user preferences and adapts its behavior over time."}),"\n",(0,i.jsx)(n.h3,{id:"requirements-2",children:"Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"User behavior modeling and prediction"}),"\n",(0,i.jsx)(n.li,{children:"Adaptive task execution"}),"\n",(0,i.jsx)(n.li,{children:"Personalized interaction patterns"}),"\n",(0,i.jsx)(n.li,{children:"Continuous learning from interactions"}),"\n",(0,i.jsx)(n.li,{children:"Preference evolution over time"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"capstone-challenge-full-home-automation-integration",children:"Capstone Challenge: Full Home Automation Integration"}),"\n",(0,i.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Combine all modules into a complete home automation system with VLA capabilities."}),"\n",(0,i.jsx)(n.h3,{id:"requirements-3",children:"Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Full integration of ROS 2, Digital Twin, AI-Robot Brain, and VLA"}),"\n",(0,i.jsx)(n.li,{children:"Seamless interaction with smart home devices"}),"\n",(0,i.jsx)(n.li,{children:"Predictive maintenance and assistance"}),"\n",(0,i.jsx)(n.li,{children:"Multi-modal interaction (voice, gesture, app)"}),"\n",(0,i.jsx)(n.li,{children:"Privacy-preserving operation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"implementation-strategy",children:"Implementation Strategy"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Phase 1"}),": Basic VLA functionality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Phase 2"}),": Integration with home devices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Phase 3"}),": Learning and adaptation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Phase 4"}),": Advanced features and optimization"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"These projects helped you practice:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Complete system integration across all modules"}),"\n",(0,i.jsx)(n.li,{children:"Complex multi-step task execution"}),"\n",(0,i.jsx)(n.li,{children:"Real-world application development"}),"\n",(0,i.jsx)(n.li,{children:"Safety and reliability considerations"}),"\n",(0,i.jsx)(n.li,{children:"User interaction design"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Completing these projects demonstrates mastery of Physical AI concepts from basic ROS 2 communication to advanced Vision-Language-Action systems. You now have the skills to develop sophisticated robotic systems that can perceive, reason, and act in physical environments."}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"After completing this curriculum, consider:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Contributing to open-source robotics projects"}),"\n",(0,i.jsx)(n.li,{children:"Pursuing advanced topics like reinforcement learning for robotics"}),"\n",(0,i.jsx)(n.li,{children:"Exploring specialized applications (industrial, healthcare, agriculture)"}),"\n",(0,i.jsx)(n.li,{children:"Developing your own Physical AI applications"}),"\n",(0,i.jsx)(n.li,{children:"Joining the robotics research community"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Congratulations on completing the Physical AI Book curriculum!"})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);