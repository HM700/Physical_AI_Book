"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[384],{7164(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4/intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Welcome to Module 4, where you\'ll integrate LLMs, speech recognition, and cognitive planning to enable humanoid robots to perform tasks based on human instructions.","source":"@site/docs/module4/intro.md","sourceDirName":"module4","slug":"/module4/intro","permalink":"/Physical-AI-Book/docs/module4/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Projects","permalink":"/Physical-AI-Book/docs/module3/projects"},"next":{"title":"Vision-Language-Action (VLA) Systems","permalink":"/Physical-AI-Book/docs/module4/vla"}}');var t=i(4848),s=i(8453);const r={sidebar_position:1},a="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Topics Covered",id:"topics-covered",level:2},{value:"Duration",id:"duration",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"Welcome to Module 4, where you'll integrate LLMs, speech recognition, and cognitive planning to enable humanoid robots to perform tasks based on human instructions."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the cutting edge of human-robot interaction, enabling robots to understand natural language commands, perceive their environment, and execute complex tasks. This module combines computer vision, natural language processing, and robotic control to create intuitive interfaces between humans and robots."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement Whisper-based speech recognition for robot commands"}),"\n",(0,t.jsx)(n.li,{children:"Integrate LLMs to translate natural language to robotic actions"}),"\n",(0,t.jsx)(n.li,{children:"Develop cognitive planning systems for task execution"}),"\n",(0,t.jsx)(n.li,{children:"Create vision-language models for perception and understanding"}),"\n",(0,t.jsx)(n.li,{children:"Build end-to-end VLA systems for complex robot behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Design intuitive human-robot interaction paradigms"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Modules 1, 2, and 3"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of deep learning concepts"}),"\n",(0,t.jsx)(n.li,{children:"Experience with natural language processing"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with computer vision techniques"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech-to-text integration using OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Large Language Model (LLM) integration for instruction parsing"}),"\n",(0,t.jsx)(n.li,{children:"Vision-language models for scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"Cognitive planning and task decomposition"}),"\n",(0,t.jsx)(n.li,{children:"Action mapping from language to robot commands"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal perception systems"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction design principles"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"duration",children:"Duration"}),"\n",(0,t.jsx)(n.p,{children:"Estimated completion time: 4-5 weeks (depending on prior experience)"}),"\n",(0,t.jsx)(n.p,{children:"Let's create the ultimate interface between human intention and robotic action!"})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);