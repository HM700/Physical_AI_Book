"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[734],{2626(e,n,o){o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module3/perception","title":"Perception Pipelines in Isaac Sim","description":"Perception is a critical component of the AI-Robot brain, enabling robots to understand their environment. Isaac Sim provides advanced tools for developing and testing perception pipelines.","source":"@site/docs/module3/perception.md","sourceDirName":"module3","slug":"/module3/perception","permalink":"/Physical-AI-Book/docs/module3/perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module3/perception.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Sim","permalink":"/Physical-AI-Book/docs/module3/isaac"},"next":{"title":"Navigation and Path Planning","permalink":"/Physical-AI-Book/docs/module3/navigation"}}');var t=o(4848),i=o(8453);const r={sidebar_position:3},a="Perception Pipelines in Isaac Sim",l={},c=[{value:"Introduction to Perception in Robotics",id:"introduction-to-perception-in-robotics",level:2},{value:"Isaac ROS Perception Components",id:"isaac-ros-perception-components",level:2},{value:"Stereo DNN Example",id:"stereo-dnn-example",level:3},{value:"Visual SLAM Implementation",id:"visual-slam-implementation",level:3},{value:"Object Detection Pipeline",id:"object-detection-pipeline",level:2},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Best Practices for Perception Systems",id:"best-practices-for-perception-systems",level:2},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"perception-pipelines-in-isaac-sim",children:"Perception Pipelines in Isaac Sim"})}),"\n",(0,t.jsx)(n.p,{children:"Perception is a critical component of the AI-Robot brain, enabling robots to understand their environment. Isaac Sim provides advanced tools for developing and testing perception pipelines."}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-perception-in-robotics",children:"Introduction to Perception in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Perception systems enable robots to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Detect and recognize objects"}),"\n",(0,t.jsx)(n.li,{children:"Understand spatial relationships"}),"\n",(0,t.jsx)(n.li,{children:"Estimate poses and trajectories"}),"\n",(0,t.jsx)(n.li,{children:"Segment scenes into meaningful parts"}),"\n",(0,t.jsx)(n.li,{children:"Process sensor data in real-time"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-perception-components",children:"Isaac ROS Perception Components"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS provides optimized perception components including:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Stereo DNN"}),"\n",(0,t.jsx)(n.li,{children:"Visual Slam"}),"\n",(0,t.jsx)(n.li,{children:"Apriltag 3D"}),"\n",(0,t.jsx)(n.li,{children:"ISAAC ROS Detection ROS"}),"\n",(0,t.jsx)(n.li,{children:"ISAAC ROS Segmentation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"stereo-dnn-example",children:"Stereo DNN Example"}),"\n",(0,t.jsx)(n.p,{children:"Stereo DNN provides real-time depth estimation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom stereo_msgs.msg import DisparityImage\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\n\nclass StereoDNNProcessor(Node):\n    def __init__(self):\n        super().__init__(\'stereo_dnn_processor\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscriptions for stereo pair\n        self.left_sub = self.create_subscription(\n            Image, \'/stereo/left/image_rect_color\', self.left_callback, 10\n        )\n        self.right_sub = self.create_subscription(\n            Image, \'/stereo/right/image_rect_color\', self.right_callback, 10\n        )\n        self.left_info_sub = self.create_subscription(\n            CameraInfo, \'/stereo/left/camera_info\', self.left_info_callback, 10\n        )\n        self.right_info_sub = self.create_subscription(\n            CameraInfo, \'/stereo/right/camera_info\', self.right_info_callback, 10\n        )\n\n        # Publisher for disparity map\n        self.disparity_pub = self.create_publisher(DisparityImage, \'/disparity_map\', 10)\n\n        # Store camera info\n        self.left_info = None\n        self.right_info = None\n\n        # Initialize stereo matching algorithm\n        self.stereo_matcher = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n    def left_callback(self, msg):\n        """Process left camera image."""\n        if self.right_image is not None:\n            # Convert ROS images to OpenCV\n            left_cv = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\n            right_cv = self.bridge.imgmsg_to_cv2(self.right_image, desired_encoding=\'passthrough\')\n\n            # Convert to grayscale for stereo matching\n            left_gray = cv2.cvtColor(left_cv, cv2.COLOR_RGB2GRAY)\n            right_gray = cv2.cvtColor(right_cv, cv2.COLOR_RGB2GRAY)\n\n            # Compute disparity\n            disparity = self.stereo_matcher.compute(left_gray, right_gray).astype(np.float32) / 16.0\n\n            # Create disparity message\n            disp_msg = DisparityImage()\n            disp_msg.header = msg.header\n            disp_msg.image = self.bridge.cv2_to_imgmsg(disparity, encoding=\'32FC1\')\n            disp_msg.f = self.left_info.K[0] if self.left_info else 1.0  # Focal length\n            disp_msg.T = 0.1  # Baseline (example value)\n\n            self.disparity_pub.publish(disp_msg)\n\n    def right_callback(self, msg):\n        """Store right camera image."""\n        self.right_image = msg\n\n    def left_info_callback(self, msg):\n        """Store left camera info."""\n        self.left_info = msg\n\n    def right_info_callback(self, msg):\n        """Store right camera info."""\n        self.right_info = msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = StereoDNNProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"visual-slam-implementation",children:"Visual SLAM Implementation"}),"\n",(0,t.jsx)(n.p,{children:"Visual SLAM enables robots to simultaneously localize themselves and map their environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom collections import deque\n\nclass VisualSLAMNode(Node):\n    def __init__(self):\n        super().__init__('visual_slam')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.camera_info_callback, 10\n        )\n\n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, '/visual_odom', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, '/visual_pose', 10)\n\n        # Store camera parameters\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n        # SLAM state\n        self.previous_frame = None\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\n        self.keyframes = []  # Store key poses\n        self.map_points = []  # Store 3D map points\n\n        # Feature detector\n        self.detector = cv2.SIFT_create(nfeatures=1000)\n        self.matcher = cv2.BFMatcher()\n\n        # Motion tracking\n        self.position = np.array([0.0, 0.0, 0.0])\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera intrinsic parameters.\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images for SLAM.\"\"\"\n        if self.camera_matrix is None:\n            return  # Wait for camera info\n\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_RGB2GRAY)\n\n        # Detect features\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n\n        if self.previous_frame is not None and descriptors is not None:\n            # Match features with previous frame\n            matches = self.matcher.knnMatch(\n                descriptors, self.previous_descriptors, k=2\n            )\n\n            # Apply Lowe's ratio test\n            good_matches = []\n            for m, n in matches:\n                if m.distance < 0.7 * n.distance:\n                    good_matches.append(m)\n\n            if len(good_matches) >= 10:  # Need sufficient matches\n                # Extract matched points\n                src_pts = np.float32([self.previous_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n                dst_pts = np.float32([keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n                # Estimate motion using Essential matrix\n                E, mask = cv2.findEssentialMat(src_pts, dst_pts, self.camera_matrix,\n                                              method=cv2.RANSAC, prob=0.999, threshold=1.0)\n\n                if E is not None:\n                    # Recover pose\n                    _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts, self.camera_matrix)\n\n                    # Update current pose\n                    delta_T = np.eye(4)\n                    delta_T[:3, :3] = R\n                    delta_T[:3, 3] = t.flatten()\n\n                    self.current_pose = self.current_pose @ delta_T\n\n                    # Extract position and orientation\n                    self.position = self.current_pose[:3, 3]\n\n                    # Convert rotation matrix to quaternion\n                    from tf_transformations import quaternion_from_matrix\n                    self.orientation = quaternion_from_matrix(self.current_pose)\n\n                    # Publish odometry\n                    self.publish_odometry(msg.header)\n\n        # Store current frame for next iteration\n        self.previous_frame = gray\n        self.previous_keypoints = keypoints\n        self.previous_descriptors = descriptors\n\n    def publish_odometry(self, header):\n        \"\"\"Publish odometry information.\"\"\"\n        # Create odometry message\n        odom_msg = Odometry()\n        odom_msg.header = header\n        odom_msg.header.frame_id = 'map'\n        odom_msg.child_frame_id = 'camera_frame'\n\n        # Set position\n        odom_msg.pose.pose.position.x = float(self.position[0])\n        odom_msg.pose.pose.position.y = float(self.position[1])\n        odom_msg.pose.pose.position.z = float(self.position[2])\n\n        # Set orientation\n        odom_msg.pose.pose.orientation.x = float(self.orientation[0])\n        odom_msg.pose.pose.orientation.y = float(self.orientation[1])\n        odom_msg.pose.pose.orientation.z = float(self.orientation[2])\n        odom_msg.pose.pose.orientation.w = float(self.orientation[3])\n\n        # Publish odometry\n        self.odom_pub.publish(odom_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    slam_node = VisualSLAMNode()\n\n    try:\n        rclpy.spin(slam_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        slam_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"object-detection-pipeline",children:"Object Detection Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Advanced object detection using Isaac Sim's synthetic data capabilities:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(Detection2DArray, '/detections', 10)\n\n        # Load pre-trained model (YOLOv5, Detectron2, etc.)\n        # In Isaac ROS, this would use optimized models\n        self.model = self.load_detection_model()\n\n        # Confidence threshold\n        self.confidence_threshold = 0.5\n\n        # Class names for Isaac Sim environment\n        self.class_names = [\n            'person', 'robot', 'table', 'chair', 'door',\n            'box', 'cone', 'cylinder', 'cube', 'plane'\n        ]\n\n    def load_detection_model(self):\n        \"\"\"Load a pre-trained object detection model.\"\"\"\n        # In Isaac ROS, this would load an optimized model\n        # For example, using Isaac ROS DNN detection\n        # model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n\n        # Placeholder for Isaac ROS optimized model\n        # This would typically be a TensorRT optimized model\n        return None\n\n    def image_callback(self, msg):\n        \"\"\"Process image for object detection.\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n\n            # Preprocess image for detection\n            input_tensor = self.preprocess_image(cv_image)\n\n            # Perform detection\n            with torch.no_grad():\n                # detections = self.model(input_tensor)\n                # Placeholder for Isaac ROS detection\n                detections = self.fake_detection(cv_image)  # Replace with actual model call\n\n            # Process detections\n            detection_array = self.process_detections(detections, msg.header)\n\n            # Publish detections\n            self.detection_pub.publish(detection_array)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in object detection: {str(e)}')\n\n    def preprocess_image(self, image):\n        \"\"\"Preprocess image for neural network.\"\"\"\n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((640, 640)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n        input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n        return input_tensor\n\n    def fake_detection(self, image):\n        \"\"\"Placeholder for actual detection - replace with Isaac ROS detection.\"\"\"\n        # This is a placeholder - in Isaac ROS, this would call the optimized detector\n        # Return mock detections for now\n        h, w = image.shape[:2]\n        return [\n            {\n                'bbox': [int(w*0.3), int(h*0.3), int(w*0.2), int(h*0.2)],  # x, y, width, height\n                'confidence': 0.8,\n                'class_id': 1,  # robot\n                'class_name': 'robot'\n            }\n        ]\n\n    def process_detections(self, detections, header):\n        \"\"\"Convert detections to ROS message format.\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for det in detections:\n            if det['confidence'] > self.confidence_threshold:\n                detection_msg = Detection2D()\n\n                # Bounding box\n                bbox = det['bbox']\n                detection_msg.bbox.center.x = float(bbox[0] + bbox[2]/2)  # center x\n                detection_msg.bbox.center.y = float(bbox[1] + bbox[3]/2)  # center y\n                detection_msg.bbox.size_x = float(bbox[2])  # width\n                detection_msg.bbox.size_y = float(bbox[3])  # height\n\n                # Classification\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.hypothesis.class_id = str(det['class_id'])\n                hypothesis.hypothesis.score = det['confidence']\n\n                detection_msg.results.append(hypothesis)\n\n                detection_array.detections.append(detection_msg)\n\n        return detection_array\n\ndef main(args=None):\n    rclpy.init(args=args)\n    detection_node = ObjectDetectionNode()\n\n    try:\n        rclpy.spin(detection_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        detection_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Semantic segmentation provides pixel-level understanding of the environment:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image as PILImage\nimport numpy as np\n\nclass SemanticSegmentationNode(Node):\n    def __init__(self):\n        super().__init__(\'semantic_segmentation\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n\n        # Publishers\n        self.segmentation_pub = self.create_publisher(Image, \'/segmentation\', 10)\n\n        # Load segmentation model\n        self.model = self.load_segmentation_model()\n\n        # Color map for visualization\n        self.color_map = self.create_color_map()\n\n    def load_segmentation_model(self):\n        """Load a pre-trained segmentation model."""\n        # In Isaac ROS, this would load an optimized segmentation model\n        # For example: DeepLab, PSPNet, etc.\n        # model = torch.hub.load(\'pytorch/vision:v0.10.0\', \'deeplabv3_resnet50\', pretrained=True)\n        # model.eval()\n        # return model\n\n        # Placeholder for Isaac ROS optimized model\n        return None\n\n    def create_color_map(self):\n        """Create a color map for different classes."""\n        # Define colors for different object classes\n        color_map = np.array([\n            [128, 64, 128],    # road\n            [244, 35, 232],    # sidewalk\n            [70, 70, 70],      # building\n            [102, 102, 156],   # wall\n            [190, 153, 153],   # fence\n            [153, 153, 153],   # pole\n            [250, 170, 30],    # traffic light\n            [220, 220, 0],     # traffic sign\n            [107, 142, 35],    # vegetation\n            [152, 251, 152],   # terrain\n            [70, 130, 180],    # sky\n            [220, 20, 60],     # person\n            [255, 0, 0],       # rider\n            [0, 0, 142],       # car\n            [0, 0, 70],        # truck\n            [0, 60, 100],      # bus\n            [0, 80, 100],      # train\n            [0, 0, 230],       # motorcycle\n            [119, 11, 32],     # bicycle\n        ], dtype=np.uint8)\n\n        return color_map\n\n    def image_callback(self, msg):\n        """Process image for semantic segmentation."""\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n\n            # Perform segmentation\n            segmented_mask = self.perform_segmentation(cv_image)\n\n            # Apply color map for visualization\n            colored_segmentation = self.apply_color_map(segmented_mask)\n\n            # Convert back to ROS image\n            seg_msg = self.bridge.cv2_to_imgmsg(colored_segmentation, encoding=\'rgb8\')\n            seg_msg.header = msg.header\n\n            # Publish segmentation\n            self.segmentation_pub.publish(seg_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in semantic segmentation: {str(e)}\')\n\n    def perform_segmentation(self, image):\n        """Perform semantic segmentation on the image."""\n        # This is a placeholder - in Isaac ROS, this would call the optimized segmenter\n        # Convert image to tensor and normalize\n        input_tensor = transforms.ToTensor()(image).unsqueeze(0)\n\n        # In Isaac ROS, this would be:\n        # with torch.no_grad():\n        #     outputs = self.model(input_tensor)\n        #     predicted = torch.argmax(outputs[\'out\'], dim=1)\n        #     return predicted.squeeze().cpu().numpy()\n\n        # For now, return a fake segmentation\n        h, w = image.shape[:2]\n        fake_mask = np.zeros((h, w), dtype=np.uint8)\n\n        # Create some fake segmentation regions\n        fake_mask[h//4:3*h//4, w//4:w//2] = 1  # Person region\n        fake_mask[3*h//4:, :] = 2  # Road region\n        fake_mask[:h//4, :] = 3  # Sky region\n\n        return fake_mask\n\n    def apply_color_map(self, mask):\n        """Apply color map to segmentation mask."""\n        # Map each class to its corresponding color\n        h, w = mask.shape\n        colored = np.zeros((h, w, 3), dtype=np.uint8)\n\n        for class_id in np.unique(mask):\n            colored[mask == class_id] = self.color_map[class_id % len(self.color_map)]\n\n        return colored\n\ndef main(args=None):\n    rclpy.init(args=args)\n    seg_node = SemanticSegmentationNode()\n\n    try:\n        rclpy.spin(seg_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        seg_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-for-perception-systems",children:"Best Practices for Perception Systems"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust Preprocessing"}),": Handle different lighting conditions and sensor noise"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Sensor Fusion"}),": Combine data from multiple sensors for better understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Performance"}),": Optimize for real-time operation requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic Data Training"}),": Use Isaac Sim's synthetic data generation for training"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validation"}),": Compare synthetic-trained models with real-world performance"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,t.jsx)(n.p,{children:"Create a complete perception pipeline that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Subscribes to RGB and depth camera data"}),"\n",(0,t.jsx)(n.li,{children:"Performs object detection and semantic segmentation"}),"\n",(0,t.jsx)(n.li,{children:"Fuses sensor data for 3D object localization"}),"\n",(0,t.jsx)(n.li,{children:"Tracks objects over time"}),"\n",(0,t.jsx)(n.li,{children:"Publishes results in standard ROS 2 message formats"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"In this section, you learned:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How to implement stereo vision and depth estimation"}),"\n",(0,t.jsx)(n.li,{children:"How to create visual SLAM systems"}),"\n",(0,t.jsx)(n.li,{children:"How to build object detection pipelines"}),"\n",(0,t.jsx)(n.li,{children:"How to implement semantic segmentation"}),"\n",(0,t.jsx)(n.li,{children:"Best practices for robust perception systems"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In the next section, we'll explore navigation and path planning in Isaac Sim."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453(e,n,o){o.d(n,{R:()=>r,x:()=>a});var s=o(6540);const t={},i=s.createContext(t);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);