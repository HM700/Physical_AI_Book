"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[384],{7164(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4/intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Welcome to Module 4, where you\'ll integrate LLMs, speech recognition, and cognitive planning to enable humanoid robots to perform tasks based on human instructions.","source":"@site/docs/module4/intro.md","sourceDirName":"module4","slug":"/module4/intro","permalink":"/Physical_AI_Book/docs/module4/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Projects","permalink":"/Physical_AI_Book/docs/module3/projects"},"next":{"title":"Vision-Language-Action (VLA) Systems","permalink":"/Physical_AI_Book/docs/module4/vla"}}');var t=i(4848),s=i(8453);const r={sidebar_position:1},l="Module 4: Vision-Language-Action (VLA)",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Topics Covered",id:"topics-covered",level:2},{value:"Duration",id:"duration",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"Welcome to Module 4, where you'll integrate LLMs, speech recognition, and cognitive planning to enable humanoid robots to perform tasks based on human instructions."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the cutting edge of human-robot interaction, enabling robots to understand natural language commands, perceive their environment, and execute complex tasks. This module combines computer vision, natural language processing, and robotic action execution."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement Whisper voice-to-text pipeline for robot commands"}),"\n",(0,t.jsx)(n.li,{children:"Integrate LLMs to map natural language to robotic actions"}),"\n",(0,t.jsx)(n.li,{children:"Develop cognitive planning systems for task execution"}),"\n",(0,t.jsx)(n.li,{children:"Create multimodal perception systems"}),"\n",(0,t.jsx)(n.li,{children:"Build end-to-end VLA systems for complex robot behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Design intuitive human-robot interaction paradigms"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Modules 1, 2, and 3"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of ROS 2, simulation environments, and AI perception"}),"\n",(0,t.jsx)(n.li,{children:"Basic knowledge of natural language processing"}),"\n",(0,t.jsx)(n.li,{children:"Experience with Python and AI/ML concepts"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech recognition with OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Large Language Model (LLM) integration for action mapping"}),"\n",(0,t.jsx)(n.li,{children:"Cognitive planning and task decomposition"}),"\n",(0,t.jsx)(n.li,{children:"Vision-language models for scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal perception and action execution"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction design principles"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"duration",children:"Duration"}),"\n",(0,t.jsx)(n.p,{children:"Estimated completion time: 4-5 weeks (depending on prior experience)"}),"\n",(0,t.jsx)(n.p,{children:"Let's build the ultimate interface between human intention and robotic action!"})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);