"use strict";(globalThis.webpackChunkphysical_ai_book_docs=globalThis.webpackChunkphysical_ai_book_docs||[]).push([[855],{5842(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4/vla","title":"Vision-Language-Action (VLA) Systems","description":"Vision-Language-Action (VLA) systems represent the integration of computer vision, natural language processing, and robotic action execution. These systems enable robots to understand human instructions, perceive their environment, and execute complex tasks.","source":"@site/docs/module4/vla.md","sourceDirName":"module4","slug":"/module4/vla","permalink":"/Physical_AI_Book/docs/module4/vla","draft":false,"unlisted":false,"editUrl":"https://github.com/HM700/Physical_AI_Book/edit/master/frontend/docs/module4/vla.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical_AI_Book/docs/module4/intro"},"next":{"title":"VLA System Integration","permalink":"/Physical_AI_Book/docs/module4/integration"}}');var s=t(4848),a=t(8453);const i={sidebar_position:2},r="Vision-Language-Action (VLA) Systems",l={},c=[{value:"Introduction to VLA Systems",id:"introduction-to-vla-systems",level:2},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:2},{value:"Setting Up Whisper",id:"setting-up-whisper",level:3},{value:"Large Language Model Integration",id:"large-language-model-integration",level:2},{value:"Cognitive Planning System",id:"cognitive-planning-system",level:2},{value:"End-to-End VLA System Integration",id:"end-to-end-vla-system-integration",level:2},{value:"Best Practices for VLA Systems",id:"best-practices-for-vla-systems",level:2},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"vision-language-action-vla-systems",children:"Vision-Language-Action (VLA) Systems"})}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language-Action (VLA) systems represent the integration of computer vision, natural language processing, and robotic action execution. These systems enable robots to understand human instructions, perceive their environment, and execute complex tasks."}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-vla-systems",children:"Introduction to VLA Systems"}),"\n",(0,s.jsx)(n.p,{children:"VLA systems combine three critical components:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"}),": Understanding the visual environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),": Interpreting natural language instructions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Executing appropriate robotic behaviors"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,s.jsx)(n.p,{children:"Whisper is OpenAI's automatic speech recognition (ASR) system that can convert spoken language to text."}),"\n",(0,s.jsx)(n.h3,{id:"setting-up-whisper",children:"Setting Up Whisper"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nimport io\nimport wave\n\nclass WhisperSpeechToTextNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_speech_to_text\')\n\n        # Subscriptions\n        self.audio_sub = self.create_subscription(\n            AudioData, \'/audio_input\', self.audio_callback, 10\n        )\n\n        # Publishers\n        self.text_pub = self.create_publisher(String, \'/speech_text\', 10)\n        self.interpretation_pub = self.create_publisher(String, \'/interpreted_command\', 10)\n\n        # Initialize Whisper model\n        # Options: tiny, base, small, medium, large\n        self.model = whisper.load_model("base")\n\n        # Audio processing parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz audio\n        self.audio_buffer = []\n        self.buffer_size = 16000  # 1 second buffer at 16kHz\n\n    def audio_callback(self, msg):\n        """Process incoming audio data."""\n        # Convert audio data to numpy array\n        audio_np = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to buffer\n        self.audio_buffer.extend(audio_np)\n\n        # Process when buffer is full\n        if len(self.audio_buffer) >= self.buffer_size:\n            # Process audio with Whisper\n            audio_segment = np.array(self.audio_buffer[:self.buffer_size])\n\n            # Transcribe using Whisper\n            result = self.model.transcribe(audio_segment)\n            transcribed_text = result["text"].strip()\n\n            if transcribed_text:  # Only publish if there\'s actual text\n                # Publish transcribed text\n                text_msg = String()\n                text_msg.data = transcribed_text\n                self.text_pub.publish(text_msg)\n\n                # Interpret the command\n                interpreted_cmd = self.interpret_command(transcribed_text)\n\n                # Publish interpreted command\n                cmd_msg = String()\n                cmd_msg.data = interpreted_cmd\n                self.interpretation_pub.publish(cmd_msg)\n\n                self.get_logger().info(f\'Speech: "{transcribed_text}" -> Command: "{interpreted_cmd}"\')\n\n            # Keep remainder of buffer for next processing\n            self.audio_buffer = self.audio_buffer[self.buffer_size:]\n\n    def interpret_command(self, text):\n        """Interpret natural language command into robot action."""\n        text_lower = text.lower()\n\n        # Simple rule-based interpretation\n        if any(word in text_lower for word in ["go to", "move to", "navigate to", "walk to"]):\n            # Extract location if possible\n            location = self.extract_location(text_lower)\n            if location:\n                return f"NAVIGATE_TO:{location}"\n            else:\n                return "NAVIGATE:DEFAULT_LOCATION"\n\n        elif any(word in text_lower for word in ["pick up", "grasp", "take", "grab"]):\n            object_name = self.extract_object(text_lower)\n            if object_name:\n                return f"PICK_UP:{object_name}"\n            else:\n                return "PICK_UP:UNKNOWN_OBJECT"\n\n        elif any(word in text_lower for word in ["put down", "place", "drop", "release"]):\n            object_name = self.extract_object(text_lower)\n            if object_name:\n                return f"PLACE:{object_name}"\n            else:\n                return "PLACE:OBJECT"\n\n        elif any(word in text_lower for word in ["follow me", "come with me", "follow"]):\n            return "FOLLOW_HUMAN:START"\n\n        elif any(word in text_lower for word in ["stop", "halt", "pause"]):\n            return "STOP:ALL_ACTIONS"\n\n        elif any(word in text_lower for word in ["find", "locate", "search for"]):\n            object_name = self.extract_object(text_lower)\n            if object_name:\n                return f"FIND_OBJECT:{object_name}"\n            else:\n                return "FIND_OBJECT:ANY"\n\n        else:\n            # Use LLM for more complex interpretation\n            return self.llm_interpret_command(text)\n\n    def extract_location(self, text):\n        """Extract location from text."""\n        # Simple location extraction\n        locations = ["kitchen", "bedroom", "living room", "office", "bathroom", "hallway", "dining room"]\n        for loc in locations:\n            if loc in text:\n                return loc.upper().replace(" ", "_")\n        return None\n\n    def extract_object(self, text):\n        """Extract object from text."""\n        # Simple object extraction\n        objects = ["cup", "book", "ball", "phone", "keys", "bottle", "box", "toy"]\n        for obj in objects:\n            if obj in text:\n                return obj.upper()\n        return None\n\n    def llm_interpret_command(self, text):\n        """Use LLM to interpret complex commands."""\n        # In a real implementation, this would call an LLM\n        # For now, return a default interpretation\n        return f"COMPLEX_COMMAND:{text[:50]}..."  # Truncate for simplicity\n\ndef main(args=None):\n    rclpy.init(args=args)\n    stt_node = WhisperSpeechToTextNode()\n\n    try:\n        rclpy.spin(stt_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        stt_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"large-language-model-integration",children:"Large Language Model Integration"}),"\n",(0,s.jsx)(n.p,{children:"Integrating LLMs to map natural language to robotic actions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport openai\nimport json\nimport re\n\nclass LLMBridgeNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_bridge\')\n\n        # Initialize OpenAI client\n        openai.api_key = "YOUR_OPENAI_API_KEY"  # Set in environment\n        self.client = openai.OpenAI()\n\n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String, \'/interpreted_command\', self.command_callback, 10\n        )\n        self.voice_command_sub = self.create_subscription(\n            String, \'/speech_text\', self.voice_command_callback, 10\n        )\n\n        # Publishers\n        self.robot_action_pub = self.create_publisher(String, \'/robot_action\', 10)\n        self.navigation_goal_pub = self.create_publisher(Pose, \'/move_base_simple/goal\', 10)\n\n        # Robot capabilities\n        self.robot_capabilities = {\n            "navigation": ["move_to", "go_to", "navigate_to", "travel_to"],\n            "manipulation": ["pick_up", "place", "grasp", "release", "take", "put"],\n            "interaction": ["greet", "follow", "wait", "stop"],\n            "perception": ["find", "locate", "identify", "describe"]\n        }\n\n        # Location map (would come from semantic map)\n        self.location_map = {\n            "kitchen": {"x": 1.0, "y": 2.0, "z": 0.0},\n            "bedroom": {"x": -1.0, "y": 3.0, "z": 0.0},\n            "living_room": {"x": 0.0, "y": 0.0, "z": 0.0},\n            "office": {"x": 2.0, "y": -1.0, "z": 0.0}\n        }\n\n    def command_callback(self, msg):\n        """Process interpreted commands."""\n        command = msg.data\n        self.process_command(command)\n\n    def voice_command_callback(self, msg):\n        """Process raw voice commands with LLM interpretation."""\n        raw_command = msg.data\n        interpreted_command = self.interpret_with_llm(raw_command)\n        self.process_command(interpreted_command)\n\n    def interpret_with_llm(self, command_text):\n        """Use LLM to interpret natural language command."""\n        system_prompt = """\n        You are a robot command interpreter. Convert natural language commands to structured robot commands.\n        Respond with a JSON object containing:\n        {\n            "action": "navigation|manipulation|interaction|perception",\n            "target": "specific object or location",\n            "parameters": {...}\n        }\n\n        Example mappings:\n        - "Go to the kitchen" -> {"action": "navigation", "target": "kitchen", "parameters": {}}\n        - "Pick up the red cup" -> {"action": "manipulation", "target": "cup", "parameters": {"color": "red"}}\n        - "Find my keys" -> {"action": "perception", "target": "keys", "parameters": {}}\n        """\n\n        try:\n            response = self.client.chat.completions.create(\n                model="gpt-3.5-turbo",\n                messages=[\n                    {"role": "system", "content": system_prompt},\n                    {"role": "user", "content": f"Command: {command_text}"}\n                ],\n                temperature=0.1,\n                max_tokens=200\n            )\n\n            # Parse the response\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON from response (in case it includes text around it)\n            json_match = re.search(r\'\\{.*\\}\', response_text, re.DOTALL)\n            if json_match:\n                json_str = json_match.group()\n                parsed = json.loads(json_str)\n\n                # Format as robot command\n                if parsed[\'action\'] == \'navigation\':\n                    return f"NAVIGATE_TO:{parsed[\'target\'].upper().replace(\' \', \'_\')}"\n                elif parsed[\'action\'] == \'manipulation\':\n                    return f"MANIPULATE:{parsed[\'target\'].upper()}:{json.dumps(parsed[\'parameters\'])}"\n                elif parsed[\'action\'] == \'perception\':\n                    return f"PERCEIVE:{parsed[\'target\'].upper()}"\n                else:\n                    return f"INTERACTION:{parsed[\'target\'].upper()}"\n            else:\n                return f"UNKNOWN_COMMAND:{command_text[:50]}"\n\n        except Exception as e:\n            self.get_logger().error(f\'LLM interpretation error: {str(e)}\')\n            return f"ERROR_INTERPRETING:{command_text[:50]}"\n\n    def process_command(self, command):\n        """Process structured robot command."""\n        self.get_logger().info(f\'Processing command: {command}\')\n\n        if command.startswith(\'NAVIGATE_TO:\'):\n            location = command.split(\':\')[1]\n            self.execute_navigation(location)\n        elif command.startswith(\'PICK_UP:\'):\n            object_name = command.split(\':\')[1]\n            self.execute_manipulation(\'pick_up\', object_name)\n        elif command.startswith(\'PLACE:\'):\n            object_name = command.split(\':\')[1]\n            self.execute_manipulation(\'place\', object_name)\n        elif command.startswith(\'FIND_OBJECT:\'):\n            object_name = command.split(\':\')[1]\n            self.execute_perception(object_name)\n        else:\n            self.get_logger().warn(f\'Unknown command format: {command}\')\n\n    def execute_navigation(self, location):\n        """Execute navigation to specified location."""\n        if location.lower() in self.location_map:\n            location_data = self.location_map[location.lower()]\n\n            # Create goal pose\n            goal_pose = Pose()\n            goal_pose.position.x = location_data[\'x\']\n            goal_pose.position.y = location_data[\'y\']\n            goal_pose.position.z = location_data[\'z\']\n            # Set orientation to face forward (z-axis)\n            goal_pose.orientation.z = 1.0\n            goal_pose.orientation.w = 0.0\n\n            # Publish navigation goal\n            self.navigation_goal_pub.publish(goal_pose)\n\n            # Publish action command\n            action_msg = String()\n            action_msg.data = f"NAVIGATE_TO:{location}"\n            self.robot_action_pub.publish(action_msg)\n\n            self.get_logger().info(f\'Navigating to {location} at ({location_data["x"]}, {location_data["y"]})\')\n        else:\n            self.get_logger().warn(f\'Unknown location: {location}\')\n\n    def execute_manipulation(self, action, object_name):\n        """Execute manipulation action."""\n        # This would interface with robot manipulator\n        action_msg = String()\n        action_msg.data = f"MANIPULATION:{action}:{object_name}"\n        self.robot_action_pub.publish(action_msg)\n\n        self.get_logger().info(f\'Executing manipulation: {action} {object_name}\')\n\n    def execute_perception(self, target_object):\n        """Execute perception task."""\n        action_msg = String()\n        action_msg.data = f"PERCEPTION:FIND:{target_object}"\n        self.robot_action_pub.publish(action_msg)\n\n        self.get_logger().info(f\'Looking for object: {target_object}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    llm_node = LLMBridgeNode()\n\n    try:\n        rclpy.spin(llm_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        llm_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"cognitive-planning-system",children:"Cognitive Planning System"}),"\n",(0,s.jsx)(n.p,{children:"Creating a cognitive planning system that reasons about tasks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom geometry_msgs.msg import Pose\nimport json\nfrom typing import List, Dict, Any\n\nclass CognitivePlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planner\')\n\n        # Subscriptions\n        self.high_level_command_sub = self.create_subscription(\n            String, \'/high_level_command\', self.command_callback, 10\n        )\n        self.action_feedback_sub = self.create_subscription(\n            String, \'/action_feedback\', self.feedback_callback, 10\n        )\n\n        # Publishers\n        self.low_level_command_pub = self.create_publisher(String, \'/low_level_command\', 10)\n        self.navigation_goal_pub = self.create_publisher(Pose, \'/move_base_simple/goal\', 10)\n        self.planning_status_pub = self.create_publisher(String, \'/planning_status\', 10)\n\n        # World model (simplified)\n        self.world_state = {\n            "locations": {\n                "kitchen": {"x": 1.0, "y": 2.0, "objects": ["cup", "plate"]},\n                "bedroom": {"x": -1.0, "y": 3.0, "objects": ["book", "keys"]},\n                "living_room": {"x": 0.0, "y": 0.0, "objects": ["sofa", "tv"]},\n                "office": {"x": 2.0, "y": -1.0, "objects": ["computer", "pen"]}\n            },\n            "robot_location": "living_room",\n            "carried_object": None,\n            "tasks_completed": []\n        }\n\n        # Task plans database\n        self.task_plans = {\n            "bring_coffee": [\n                {"action": "navigate", "target": "kitchen"},\n                {"action": "find_object", "target": "cup"},\n                {"action": "grasp", "target": "cup"},\n                {"action": "navigate", "target": "living_room"},\n                {"action": "place", "target": "table"}\n            ],\n            "find_keys": [\n                {"action": "navigate", "target": "bedroom"},\n                {"action": "find_object", "target": "keys"},\n                {"action": "report_location", "target": "keys"}\n            ],\n            "clean_room": [\n                {"action": "navigate", "target": "living_room"},\n                {"action": "find_object", "target": "trash"},\n                {"action": "collect_trash", "target": "all"},\n                {"action": "navigate", "target": "kitchen"},\n                {"action": "dispose_trash", "target": "bin"}\n            ]\n        }\n\n        # Current plan and execution state\n        self.current_plan = []\n        self.current_step_index = 0\n        self.planning_state = "IDLE"  # IDLE, PLANNING, EXECUTING, FAILED, COMPLETED\n\n    def command_callback(self, msg):\n        """Process high-level commands."""\n        command = msg.data\n        self.get_logger().info(f\'Received high-level command: {command}\')\n\n        # Determine task type\n        if command.startswith(\'BRING_\'):\n            self.plan_and_execute_task(\'bring_coffee\')\n        elif command.startswith(\'FIND_\'):\n            self.plan_and_execute_task(\'find_keys\')\n        elif command.startswith(\'CLEAN_\'):\n            self.plan_and_execute_task(\'clean_room\')\n        else:\n            # Try to determine task from command\n            task_type = self.infer_task_type(command)\n            if task_type:\n                self.plan_and_execute_task(task_type)\n\n    def infer_task_type(self, command):\n        """Infer task type from command."""\n        command_lower = command.lower()\n\n        if any(word in command_lower for word in ["bring", "get", "fetch", "carry"]):\n            return "bring_coffee"  # Simplified mapping\n        elif any(word in command_lower for word in ["find", "locate", "search"]):\n            return "find_keys"\n        elif any(word in command_lower for word in ["clean", "tidy", "organize"]):\n            return "clean_room"\n        else:\n            return None\n\n    def plan_and_execute_task(self, task_type):\n        """Plan and execute a task."""\n        if task_type in self.task_plans:\n            self.current_plan = self.task_plans[task_type]\n            self.current_step_index = 0\n            self.planning_state = "EXECUTING"\n\n            self.get_logger().info(f\'Starting task: {task_type} with {len(self.current_plan)} steps\')\n\n            # Execute first step\n            if self.current_plan:\n                self.execute_next_step()\n        else:\n            self.get_logger().error(f\'Unknown task type: {task_type}\')\n            self.planning_state = "FAILED"\n\n    def execute_next_step(self):\n        """Execute the next step in the current plan."""\n        if self.current_step_index >= len(self.current_plan):\n            # Plan completed\n            self.planning_state = "COMPLETED"\n            self.publish_status(f"TASK_COMPLETED:{self.current_plan[0][\'action\']}")\n            return\n\n        step = self.current_plan[self.current_step_index]\n        self.get_logger().info(f\'Executing step {self.current_step_index + 1}: {step["action"]} {step.get("target", "")}\')\n\n        # Execute based on action type\n        if step["action"] == "navigate":\n            self.execute_navigation(step["target"])\n        elif step["action"] == "find_object":\n            self.execute_find_object(step["target"])\n        elif step["action"] == "grasp":\n            self.execute_grasp(step["target"])\n        elif step["action"] == "place":\n            self.execute_place(step["target"])\n        elif step["action"] == "report_location":\n            self.execute_report_location(step["target"])\n        elif step["action"] == "collect_trash":\n            self.execute_collect_trash(step["target"])\n        elif step["action"] == "dispose_trash":\n            self.execute_dispose_trash(step["target"])\n        else:\n            self.get_logger().error(f\'Unknown action: {step["action"]}\')\n            self.planning_state = "FAILED"\n\n    def execute_navigation(self, location):\n        """Execute navigation to location."""\n        if location in self.world_state["locations"]:\n            loc_data = self.world_state["locations"][location]\n\n            # Create and publish navigation goal\n            goal_pose = Pose()\n            goal_pose.position.x = loc_data["x"]\n            goal_pose.position.y = loc_data["y"]\n            goal_pose.position.z = 0.0\n            goal_pose.orientation.z = 0.0\n            goal_pose.orientation.w = 1.0\n\n            self.navigation_goal_pub.publish(goal_pose)\n\n            # Update world state\n            self.world_state["robot_location"] = location\n\n            # Publish command\n            cmd_msg = String()\n            cmd_msg.data = f"NAVIGATE:{location}"\n            self.low_level_command_pub.publish(cmd_msg)\n        else:\n            self.get_logger().error(f\'Location not found: {location}\')\n            self.planning_state = "FAILED"\n\n    def execute_find_object(self, target_object):\n        """Execute object finding."""\n        current_location = self.world_state["robot_location"]\n        location_objects = self.world_state["locations"][current_location]["objects"]\n\n        if target_object.lower() in [obj.lower() for obj in location_objects]:\n            # Object found\n            self.get_logger().info(f\'Found {target_object} in {current_location}\')\n\n            cmd_msg = String()\n            cmd_msg.data = f"FOUND_OBJECT:{target_object}_IN_{current_location.upper().replace(\' \', \'_\')}"\n            self.low_level_command_pub.publish(cmd_msg)\n        else:\n            # Object not in current location\n            self.get_logger().info(f\'{target_object} not found in {current_location}\')\n\n            # Could implement search in other locations\n            cmd_msg = String()\n            cmd_msg.data = f"OBJECT_NOT_FOUND:{target_object}"\n            self.low_level_command_pub.publish(cmd_msg)\n            self.planning_state = "FAILED"\n\n    def execute_grasp(self, target_object):\n        """Execute grasping action."""\n        current_location = self.world_state["robot_location"]\n        location_objects = self.world_state["locations"][current_location]["objects"]\n\n        if target_object.lower() in [obj.lower() for obj in location_objects]:\n            # Remove object from location and add to carried\n            location_objects.remove(target_object.lower())\n            self.world_state["carried_object"] = target_object\n\n            cmd_msg = String()\n            cmd_msg.data = f"GRASP_SUCCESS:{target_object}"\n            self.low_level_command_pub.publish(cmd_msg)\n        else:\n            cmd_msg = String()\n            cmd_msg.data = f"GRASP_FAILED:{target_object}_NOT_PRESENT"\n            self.low_level_command_pub.publish(cmd_msg)\n            self.planning_state = "FAILED"\n\n    def execute_place(self, target_location):\n        """Execute placing action."""\n        if self.world_state["carried_object"]:\n            carried_obj = self.world_state["carried_object"]\n\n            # Add object to target location\n            if target_location in self.world_state["locations"]:\n                self.world_state["locations"][target_location]["objects"].append(carried_obj.lower())\n                self.world_state["carried_object"] = None\n\n                cmd_msg = String()\n                cmd_msg.data = f"PLACE_SUCCESS:{carried_obj}_AT_{target_location.upper().replace(\' \', \'_\')}"\n                self.low_level_command_pub.publish(cmd_msg)\n            else:\n                cmd_msg = String()\n                cmd_msg.data = f"PLACE_FAILED:UNKNOWN_LOCATION_{target_location}"\n                self.low_level_command_pub.publish(cmd_msg)\n                self.planning_state = "FAILED"\n        else:\n            cmd_msg = String()\n            cmd_msg.data = f"PLACE_FAILED:NO_OBJECT_CARRIED"\n            self.low_level_command_pub.publish(cmd_msg)\n            self.planning_state = "FAILED"\n\n    def feedback_callback(self, msg):\n        """Process action feedback."""\n        feedback = msg.data\n\n        if feedback.startswith("ACTION_COMPLETED:"):\n            # Move to next step\n            self.current_step_index += 1\n            self.execute_next_step()\n        elif feedback.startswith("ACTION_FAILED:"):\n            # Handle failure\n            self.planning_state = "FAILED"\n            self.publish_status(f"TASK_FAILED:{feedback}")\n        elif feedback.startswith("NAVIGATION_COMPLETE"):\n            # Navigation completed, move to next step\n            self.current_step_index += 1\n            self.execute_next_step()\n\n    def publish_status(self, status):\n        """Publish planning status."""\n        status_msg = String()\n        status_msg.data = status\n        self.planning_status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = CognitivePlannerNode()\n\n    # Example: trigger a high-level command\n    # This would normally come from voice processing or high-level system\n    def trigger_example():\n        cmd_msg = String()\n        cmd_msg.data = "BRING_COFFEE"\n        planner.high_level_command_pub.publish(cmd_msg)\n\n    # Schedule example after a delay\n    planner.create_timer(2.0, trigger_example)\n\n    try:\n        rclpy.spin(planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"end-to-end-vla-system-integration",children:"End-to-End VLA System Integration"}),"\n",(0,s.jsx)(n.p,{children:"Combining all components into a complete VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Pose\nfrom audio_common_msgs.msg import AudioData\nimport threading\nimport queue\n\nclass VLASystemNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_system\')\n\n        # Initialize components\n        self.whisper_node = WhisperSpeechToTextNode(self)\n        self.llm_node = LLMBridgeNode(self)\n        self.planner_node = CognitivePlannerNode(self)\n\n        # Subscriptions for monitoring\n        self.vla_status_sub = self.create_subscription(\n            String, \'/vla_system_status\', self.status_callback, 10\n        )\n\n        # Publishers\n        self.system_status_pub = self.create_publisher(String, \'/vla_system_status\', 10)\n\n        # Internal queues for inter-component communication\n        self.command_queue = queue.Queue()\n        self.feedback_queue = queue.Queue()\n\n        # System state\n        self.system_state = "ACTIVE"  # ACTIVE, STANDBY, ERROR\n\n        # Start processing threads\n        self.processing_thread = threading.Thread(target=self.process_commands, daemon=True)\n        self.processing_thread.start()\n\n        # Timer for system monitoring\n        self.monitor_timer = self.create_timer(1.0, self.monitor_system)\n\n    def status_callback(self, msg):\n        """Receive status updates from components."""\n        status = msg.data\n        self.get_logger().info(f\'Component status: {status}\')\n\n    def process_commands(self):\n        """Process commands in separate thread."""\n        while rclpy.ok() and self.system_state == "ACTIVE":\n            try:\n                if not self.command_queue.empty():\n                    command = self.command_queue.get_nowait()\n                    self.process_vla_command(command)\n            except queue.Empty:\n                pass\n            except Exception as e:\n                self.get_logger().error(f\'Error in command processing: {str(e)}\')\n\n    def process_vla_command(self, command):\n        """Process a VLA command through the pipeline."""\n        self.get_logger().info(f\'Processing VLA command: {command}\')\n\n        # This would coordinate the flow:\n        # 1. Speech to text (handled by whisper node)\n        # 2. Language understanding (handled by llm node)\n        # 3. Task planning (handled by planner node)\n        # 4. Action execution (handled by robot controllers)\n\n        # For now, just pass through\n        pass\n\n    def monitor_system(self):\n        """Monitor system health."""\n        # Check if all components are running\n        # Publish system status\n        status_msg = String()\n        status_msg.data = f"VLA_SYSTEM_STATUS:{self.system_state}:COMPONENTS_OK"\n        self.system_status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_system = VLASystemNode()\n\n    try:\n        rclpy.spin(vla_system)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-vla-systems",children:"Best Practices for VLA Systems"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust Speech Recognition"}),": Handle noisy environments and various accents"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Awareness"}),": Maintain context across multiple interactions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Failure Handling"}),": Implement graceful degradation when components fail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety First"}),": Ensure all actions are safe before execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-in-the-Loop"}),": Provide ways for humans to intervene"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,s.jsx)(n.p,{children:"Create a complete VLA system that:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Accepts voice commands through Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Interprets commands using an LLM"}),"\n",(0,s.jsx)(n.li,{children:"Plans multi-step tasks cognitively"}),"\n",(0,s.jsx)(n.li,{children:"Executes actions safely in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Provides feedback and handles errors gracefully"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this section, you learned:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How to integrate Whisper for speech-to-text"}),"\n",(0,s.jsx)(n.li,{children:"How to use LLMs for language understanding"}),"\n",(0,s.jsx)(n.li,{children:"How to create cognitive planning systems"}),"\n",(0,s.jsx)(n.li,{children:"How to combine all components into a VLA system"}),"\n",(0,s.jsx)(n.li,{children:"Best practices for robust VLA implementations"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In the next section, we'll explore projects that combine all VLA concepts."})]})}function _(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);